{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ratings_train = pd.read_csv(\"./Data/rate_train.csv\", low_memory=False)\n",
    "ratings_test = pd.read_csv(\"./Data/rate_test.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets_train = ratings_train.Node_Id.unique()\n",
    "model_train = ratings_train.Model_Id.unique()\n",
    "datasets_test = ratings_test.Node_Id.unique()\n",
    "model_test = ratings_test.Model_Id.unique()\n",
    "meta_models = pd.read_csv(\"./Data/model_v.csv\",low_memory=False)\n",
    "models = meta_models.model_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Rating Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_model_train_matrix = pd.DataFrame(index=datasets_train,columns=models)\n",
    "data_model_test_matrix = pd.DataFrame(index=datasets_test,columns=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for row in ratings_train.itertuples():\n",
    "    data_model_train_matrix.loc[row[1]][row[2]] = row[3]\n",
    "data_model_train_matrix = data_model_train_matrix.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for row in ratings_test.itertuples():\n",
    "    data_model_test_matrix.loc[row[1]][row[2]] = row[3]\n",
    "data_model_test_matrix = data_model_test_matrix.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          289       290       291       292       293       294       295  \\\n15   0.380952  0.285714  0.615385  0.363636  0.615385  0.363636  0.615385   \n39   0.421053  0.200000  0.666667  0.222222  0.666667  0.222222  0.666667   \n9    0.434783  0.363636  0.833333  0.444444  0.833333  0.444444  0.833333   \n22   0.200000  0.210526  0.235294  0.235294  0.235294  0.235294  0.235294   \n128  0.285714  0.285714  0.285714  0.285714  0.285714  0.285714  0.285714   \n103  0.666667  0.333333  0.666667  0.333333  0.666667  0.333333  0.666667   \n116  0.800000  0.285714  0.750000  0.333333  0.750000  0.333333  0.750000   \n113  0.444444  0.250000  0.571429  0.333333  0.571429  0.333333  0.571429   \n111  0.285714  0.333333  0.333333  0.333333  0.333333  0.333333  0.333333   \n138  0.608696  0.333333  0.823529  0.333333  0.750000  0.333333  0.750000   \n89   0.555556  0.400000  0.769231  0.400000  0.769231  0.400000  0.769231   \n88   0.571429  0.400000  0.666667  0.400000  0.666667  0.400000  0.666667   \n161  0.222222  0.250000  0.250000  0.250000  0.250000  0.250000  0.250000   \n171  0.428571  0.285714  0.857143  0.400000  0.857143  0.400000  0.857143   \n234  0.222222  0.285714  0.285714  0.285714  0.285714  0.285714  0.285714   \n195  0.285714  0.285714  0.285714  0.285714  0.285714  0.285714  0.285714   \n143  0.400000  0.400000  0.400000  0.400000  0.400000  0.400000  0.400000   \n235  0.500000  0.222222  0.400000  0.222222  0.400000  0.222222  0.400000   \n168  0.428571  0.250000  0.857143  0.400000  0.857143  0.400000  0.857143   \n154  0.250000  0.285714  0.285714  0.285714  0.285714  0.285714  0.285714   \n55   0.736842  0.363636  0.800000  0.363636  0.714286  0.363636  0.714286   \n69   0.444444  0.333333  0.800000  0.500000  0.800000  0.500000  0.800000   \n222  0.250000  0.250000  0.285714  0.285714  0.285714  0.285714  0.285714   \n79   0.222222  0.285714  0.333333  0.333333  0.333333  0.333333  0.333333   \n178  0.761905  0.363636  0.875000  0.363636  0.800000  0.363636  0.800000   \n93   0.600000  0.444444  0.769231  0.500000  0.833333  0.500000  0.833333   \n91   0.500000  0.363636  0.615385  0.363636  0.615385  0.363636  0.615385   \n146  0.285714  0.285714  0.285714  0.285714  0.285714  0.285714  0.285714   \n90   0.333333  0.400000  0.400000  0.400000  0.400000  0.400000  0.400000   \n53   0.285714  0.333333  0.333333  0.333333  0.333333  0.333333  0.333333   \n224  0.222222  0.250000  0.250000  0.250000  0.250000  0.250000  0.250000   \n122  0.600000  0.333333  0.750000  0.333333  0.750000  0.333333  0.750000   \n136  0.666667  0.285714  0.857143  0.400000  0.857143  0.400000  0.857143   \n137  0.521739  0.333333  0.750000  0.333333  0.750000  0.333333  0.750000   \n246  0.228898  0.013514  0.023649  0.006814  0.016949  0.003413  0.016949   \n257  0.009335  0.002342  0.007009  0.002342  0.007009  0.002342  0.007009   \n267  0.125813  0.004640  0.013857  0.004640  0.013857  0.004640  0.009259   \n282  0.076087  0.109091  0.103448  0.040816  0.107143  0.040816  0.109091   \n283  0.016077  0.003236  0.012882  0.003236  0.012882  0.003236  0.012882   \n284  0.016807  0.003384  0.013468  0.003384  0.013468  0.003384  0.013468   \n\n          296       297       298  ...       741       742       743  \\\n15   0.363636  0.615385  0.363636  ...  0.000000  0.000000  0.000000   \n39   0.222222  0.666667  0.222222  ...  0.000000  0.000000  0.000000   \n9    0.444444  0.833333  0.444444  ...  0.000000  0.000000  0.000000   \n22   0.235294  0.235294  0.235294  ...  0.000000  0.000000  0.000000   \n128  0.285714  0.285714  0.285714  ...  0.000000  0.000000  0.000000   \n103  0.333333  0.666667  0.333333  ...  0.000000  0.000000  0.000000   \n116  0.333333  0.750000  0.333333  ...  0.000000  0.000000  0.000000   \n113  0.333333  0.571429  0.333333  ...  0.000000  0.000000  0.000000   \n111  0.333333  0.333333  0.333333  ...  0.000000  0.000000  0.000000   \n138  0.333333  0.750000  0.333333  ...  0.000000  0.000000  0.000000   \n89   0.400000  0.769231  0.400000  ...  0.000000  0.000000  0.000000   \n88   0.400000  0.666667  0.400000  ...  0.000000  0.000000  0.000000   \n161  0.250000  0.250000  0.250000  ...  0.000000  0.000000  0.000000   \n171  0.400000  0.857143  0.400000  ...  0.000000  0.000000  0.000000   \n234  0.285714  0.285714  0.285714  ...  0.000000  0.000000  0.000000   \n195  0.285714  0.285714  0.285714  ...  0.000000  0.000000  0.000000   \n143  0.400000  0.400000  0.400000  ...  0.000000  0.000000  0.000000   \n235  0.222222  0.222222  0.222222  ...  0.000000  0.000000  0.000000   \n168  0.400000  0.857143  0.400000  ...  0.000000  0.000000  0.000000   \n154  0.285714  0.285714  0.285714  ...  0.000000  0.000000  0.000000   \n55   0.363636  0.714286  0.363636  ...  0.000000  0.000000  0.000000   \n69   0.500000  0.800000  0.500000  ...  0.000000  0.000000  0.000000   \n222  0.285714  0.285714  0.285714  ...  0.000000  0.000000  0.000000   \n79   0.333333  0.333333  0.333333  ...  0.000000  0.000000  0.000000   \n178  0.363636  0.800000  0.363636  ...  0.000000  0.000000  0.000000   \n93   0.500000  0.833333  0.500000  ...  0.000000  0.000000  0.000000   \n91   0.363636  0.615385  0.363636  ...  0.000000  0.000000  0.000000   \n146  0.285714  0.285714  0.285714  ...  0.000000  0.000000  0.000000   \n90   0.400000  0.400000  0.400000  ...  0.000000  0.000000  0.000000   \n53   0.333333  0.333333  0.333333  ...  0.000000  0.000000  0.000000   \n224  0.250000  0.250000  0.250000  ...  0.000000  0.000000  0.000000   \n122  0.333333  0.750000  0.333333  ...  0.000000  0.000000  0.000000   \n136  0.400000  0.857143  0.400000  ...  0.000000  0.000000  0.000000   \n137  0.333333  0.750000  0.333333  ...  0.000000  0.000000  0.000000   \n246  0.003413  0.016949  0.003413  ...  0.016949  0.016949  0.016949   \n257  0.002342  0.007009  0.002342  ...  0.013970  0.013970  0.013970   \n267  0.004640  0.009259  0.004640  ...  0.018433  0.018433  0.018433   \n282  0.040816  0.109091  0.040816  ...  0.109091  0.109091  0.111111   \n283  0.003236  0.012882  0.003236  ...  0.022436  0.022436  0.022436   \n284  0.003384  0.013468  0.003384  ...  0.023451  0.023451  0.023451   \n\n          744       745       746       747       748       749       750  \n15   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n39   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n9    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n22   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n128  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n103  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n116  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n113  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n111  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n138  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n89   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n88   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n161  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n171  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n234  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n195  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n143  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n235  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n168  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n154  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n55   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n69   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n222  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n79   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n178  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n93   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n91   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n146  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n90   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n53   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n224  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n122  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n136  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n137  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n246  0.016949  0.016949  0.016949  0.013582  0.013582  0.013582  0.013582  \n257  0.013970  0.013970  0.013970  0.013970  0.013970  0.013970  0.013970  \n267  0.018433  0.018433  0.018433  0.018433  0.018433  0.018433  0.018433  \n282  0.111111  0.111111  0.111111  0.113208  0.113208  0.113208  0.113208  \n283  0.022436  0.022436  0.022436  0.019262  0.019262  0.019262  0.019262  \n284  0.023451  0.023451  0.023451  0.023451  0.023451  0.023451  0.023451  \n\n[40 rows x 462 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>289</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>...</th>\n      <th>741</th>\n      <th>742</th>\n      <th>743</th>\n      <th>744</th>\n      <th>745</th>\n      <th>746</th>\n      <th>747</th>\n      <th>748</th>\n      <th>749</th>\n      <th>750</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15</th>\n      <td>0.380952</td>\n      <td>0.285714</td>\n      <td>0.615385</td>\n      <td>0.363636</td>\n      <td>0.615385</td>\n      <td>0.363636</td>\n      <td>0.615385</td>\n      <td>0.363636</td>\n      <td>0.615385</td>\n      <td>0.363636</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.421053</td>\n      <td>0.200000</td>\n      <td>0.666667</td>\n      <td>0.222222</td>\n      <td>0.666667</td>\n      <td>0.222222</td>\n      <td>0.666667</td>\n      <td>0.222222</td>\n      <td>0.666667</td>\n      <td>0.222222</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.434783</td>\n      <td>0.363636</td>\n      <td>0.833333</td>\n      <td>0.444444</td>\n      <td>0.833333</td>\n      <td>0.444444</td>\n      <td>0.833333</td>\n      <td>0.444444</td>\n      <td>0.833333</td>\n      <td>0.444444</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.200000</td>\n      <td>0.210526</td>\n      <td>0.235294</td>\n      <td>0.235294</td>\n      <td>0.235294</td>\n      <td>0.235294</td>\n      <td>0.235294</td>\n      <td>0.235294</td>\n      <td>0.235294</td>\n      <td>0.235294</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>0.800000</td>\n      <td>0.285714</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>0.444444</td>\n      <td>0.250000</td>\n      <td>0.571429</td>\n      <td>0.333333</td>\n      <td>0.571429</td>\n      <td>0.333333</td>\n      <td>0.571429</td>\n      <td>0.333333</td>\n      <td>0.571429</td>\n      <td>0.333333</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>0.285714</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>0.608696</td>\n      <td>0.333333</td>\n      <td>0.823529</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>0.555556</td>\n      <td>0.400000</td>\n      <td>0.769231</td>\n      <td>0.400000</td>\n      <td>0.769231</td>\n      <td>0.400000</td>\n      <td>0.769231</td>\n      <td>0.400000</td>\n      <td>0.769231</td>\n      <td>0.400000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>0.571429</td>\n      <td>0.400000</td>\n      <td>0.666667</td>\n      <td>0.400000</td>\n      <td>0.666667</td>\n      <td>0.400000</td>\n      <td>0.666667</td>\n      <td>0.400000</td>\n      <td>0.666667</td>\n      <td>0.400000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>0.222222</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>0.428571</td>\n      <td>0.285714</td>\n      <td>0.857143</td>\n      <td>0.400000</td>\n      <td>0.857143</td>\n      <td>0.400000</td>\n      <td>0.857143</td>\n      <td>0.400000</td>\n      <td>0.857143</td>\n      <td>0.400000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>234</th>\n      <td>0.222222</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>235</th>\n      <td>0.500000</td>\n      <td>0.222222</td>\n      <td>0.400000</td>\n      <td>0.222222</td>\n      <td>0.400000</td>\n      <td>0.222222</td>\n      <td>0.400000</td>\n      <td>0.222222</td>\n      <td>0.222222</td>\n      <td>0.222222</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>168</th>\n      <td>0.428571</td>\n      <td>0.250000</td>\n      <td>0.857143</td>\n      <td>0.400000</td>\n      <td>0.857143</td>\n      <td>0.400000</td>\n      <td>0.857143</td>\n      <td>0.400000</td>\n      <td>0.857143</td>\n      <td>0.400000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>0.250000</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>0.736842</td>\n      <td>0.363636</td>\n      <td>0.800000</td>\n      <td>0.363636</td>\n      <td>0.714286</td>\n      <td>0.363636</td>\n      <td>0.714286</td>\n      <td>0.363636</td>\n      <td>0.714286</td>\n      <td>0.363636</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>0.444444</td>\n      <td>0.333333</td>\n      <td>0.800000</td>\n      <td>0.500000</td>\n      <td>0.800000</td>\n      <td>0.500000</td>\n      <td>0.800000</td>\n      <td>0.500000</td>\n      <td>0.800000</td>\n      <td>0.500000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>222</th>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>0.222222</td>\n      <td>0.285714</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>178</th>\n      <td>0.761905</td>\n      <td>0.363636</td>\n      <td>0.875000</td>\n      <td>0.363636</td>\n      <td>0.800000</td>\n      <td>0.363636</td>\n      <td>0.800000</td>\n      <td>0.363636</td>\n      <td>0.800000</td>\n      <td>0.363636</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>0.600000</td>\n      <td>0.444444</td>\n      <td>0.769231</td>\n      <td>0.500000</td>\n      <td>0.833333</td>\n      <td>0.500000</td>\n      <td>0.833333</td>\n      <td>0.500000</td>\n      <td>0.833333</td>\n      <td>0.500000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>0.500000</td>\n      <td>0.363636</td>\n      <td>0.615385</td>\n      <td>0.363636</td>\n      <td>0.615385</td>\n      <td>0.363636</td>\n      <td>0.615385</td>\n      <td>0.363636</td>\n      <td>0.615385</td>\n      <td>0.363636</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>0.333333</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>0.285714</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>224</th>\n      <td>0.222222</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>0.600000</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>136</th>\n      <td>0.666667</td>\n      <td>0.285714</td>\n      <td>0.857143</td>\n      <td>0.400000</td>\n      <td>0.857143</td>\n      <td>0.400000</td>\n      <td>0.857143</td>\n      <td>0.400000</td>\n      <td>0.857143</td>\n      <td>0.400000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>0.521739</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>0.333333</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>0.228898</td>\n      <td>0.013514</td>\n      <td>0.023649</td>\n      <td>0.006814</td>\n      <td>0.016949</td>\n      <td>0.003413</td>\n      <td>0.016949</td>\n      <td>0.003413</td>\n      <td>0.016949</td>\n      <td>0.003413</td>\n      <td>...</td>\n      <td>0.016949</td>\n      <td>0.016949</td>\n      <td>0.016949</td>\n      <td>0.016949</td>\n      <td>0.016949</td>\n      <td>0.016949</td>\n      <td>0.013582</td>\n      <td>0.013582</td>\n      <td>0.013582</td>\n      <td>0.013582</td>\n    </tr>\n    <tr>\n      <th>257</th>\n      <td>0.009335</td>\n      <td>0.002342</td>\n      <td>0.007009</td>\n      <td>0.002342</td>\n      <td>0.007009</td>\n      <td>0.002342</td>\n      <td>0.007009</td>\n      <td>0.002342</td>\n      <td>0.007009</td>\n      <td>0.002342</td>\n      <td>...</td>\n      <td>0.013970</td>\n      <td>0.013970</td>\n      <td>0.013970</td>\n      <td>0.013970</td>\n      <td>0.013970</td>\n      <td>0.013970</td>\n      <td>0.013970</td>\n      <td>0.013970</td>\n      <td>0.013970</td>\n      <td>0.013970</td>\n    </tr>\n    <tr>\n      <th>267</th>\n      <td>0.125813</td>\n      <td>0.004640</td>\n      <td>0.013857</td>\n      <td>0.004640</td>\n      <td>0.013857</td>\n      <td>0.004640</td>\n      <td>0.009259</td>\n      <td>0.004640</td>\n      <td>0.009259</td>\n      <td>0.004640</td>\n      <td>...</td>\n      <td>0.018433</td>\n      <td>0.018433</td>\n      <td>0.018433</td>\n      <td>0.018433</td>\n      <td>0.018433</td>\n      <td>0.018433</td>\n      <td>0.018433</td>\n      <td>0.018433</td>\n      <td>0.018433</td>\n      <td>0.018433</td>\n    </tr>\n    <tr>\n      <th>282</th>\n      <td>0.076087</td>\n      <td>0.109091</td>\n      <td>0.103448</td>\n      <td>0.040816</td>\n      <td>0.107143</td>\n      <td>0.040816</td>\n      <td>0.109091</td>\n      <td>0.040816</td>\n      <td>0.109091</td>\n      <td>0.040816</td>\n      <td>...</td>\n      <td>0.109091</td>\n      <td>0.109091</td>\n      <td>0.111111</td>\n      <td>0.111111</td>\n      <td>0.111111</td>\n      <td>0.111111</td>\n      <td>0.113208</td>\n      <td>0.113208</td>\n      <td>0.113208</td>\n      <td>0.113208</td>\n    </tr>\n    <tr>\n      <th>283</th>\n      <td>0.016077</td>\n      <td>0.003236</td>\n      <td>0.012882</td>\n      <td>0.003236</td>\n      <td>0.012882</td>\n      <td>0.003236</td>\n      <td>0.012882</td>\n      <td>0.003236</td>\n      <td>0.012882</td>\n      <td>0.003236</td>\n      <td>...</td>\n      <td>0.022436</td>\n      <td>0.022436</td>\n      <td>0.022436</td>\n      <td>0.022436</td>\n      <td>0.022436</td>\n      <td>0.022436</td>\n      <td>0.019262</td>\n      <td>0.019262</td>\n      <td>0.019262</td>\n      <td>0.019262</td>\n    </tr>\n    <tr>\n      <th>284</th>\n      <td>0.016807</td>\n      <td>0.003384</td>\n      <td>0.013468</td>\n      <td>0.003384</td>\n      <td>0.013468</td>\n      <td>0.003384</td>\n      <td>0.013468</td>\n      <td>0.003384</td>\n      <td>0.013468</td>\n      <td>0.003384</td>\n      <td>...</td>\n      <td>0.023451</td>\n      <td>0.023451</td>\n      <td>0.023451</td>\n      <td>0.023451</td>\n      <td>0.023451</td>\n      <td>0.023451</td>\n      <td>0.023451</td>\n      <td>0.023451</td>\n      <td>0.023451</td>\n      <td>0.023451</td>\n    </tr>\n  </tbody>\n</table>\n<p>40 rows × 462 columns</p>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_model_test_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dataset Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "meta_datasets = pd.read_csv(\"./Data/dataset_v.csv\",low_memory=False)\n",
    "datasets = meta_datasets.dataset_id.unique()\n",
    "meta_datasets = meta_datasets.loc[:,(\"v1\",\"v2\",\"v3\",\"v4\",\"v5\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "meta_dataset_similarity = cosine_similarity(meta_datasets.values.tolist())\n",
    "meta_dataset_similarity = pd.DataFrame(meta_dataset_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "     model_id  model_type        v1        v2        v3        v4  distance  \\\n0         289           0  0.124729  0.109505 -0.132957  0.163413         1   \n1         290           0  0.124729  0.109505 -0.132957  0.163413         1   \n2         291           0  0.124729  0.109505 -0.132957  0.163413       100   \n3         292           0  0.124729  0.109505 -0.132957  0.163413       100   \n4         293           0  0.124729  0.109505 -0.132957  0.163413       150   \n..        ...         ...       ...       ...       ...       ...       ...   \n457       746           1  0.123656  0.107174 -0.123199  0.164318       350   \n458       747           1  0.123656  0.107174 -0.123199  0.164318       350   \n459       748           1  0.123656  0.107174 -0.123199  0.164318       350   \n460       749           1  0.123656  0.107174 -0.123199  0.164318       350   \n461       750           1  0.123656  0.107174 -0.123199  0.164318       350   \n\n     threshold  prominence  height  width  \n0          0.3           0       0      0  \n1          0.7           0       0      0  \n2          0.3           0       0      0  \n3          0.7           0       0      0  \n4          0.3           0       0      0  \n..         ...         ...     ...    ...  \n457        0.5         400      50      5  \n458        0.0        1000      50      0  \n459        0.0        1000      50      5  \n460        0.5        1000      50      0  \n461        0.5        1000      50      5  \n\n[462 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model_id</th>\n      <th>model_type</th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>v3</th>\n      <th>v4</th>\n      <th>distance</th>\n      <th>threshold</th>\n      <th>prominence</th>\n      <th>height</th>\n      <th>width</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>289</td>\n      <td>0</td>\n      <td>0.124729</td>\n      <td>0.109505</td>\n      <td>-0.132957</td>\n      <td>0.163413</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>290</td>\n      <td>0</td>\n      <td>0.124729</td>\n      <td>0.109505</td>\n      <td>-0.132957</td>\n      <td>0.163413</td>\n      <td>1</td>\n      <td>0.7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>291</td>\n      <td>0</td>\n      <td>0.124729</td>\n      <td>0.109505</td>\n      <td>-0.132957</td>\n      <td>0.163413</td>\n      <td>100</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>292</td>\n      <td>0</td>\n      <td>0.124729</td>\n      <td>0.109505</td>\n      <td>-0.132957</td>\n      <td>0.163413</td>\n      <td>100</td>\n      <td>0.7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>293</td>\n      <td>0</td>\n      <td>0.124729</td>\n      <td>0.109505</td>\n      <td>-0.132957</td>\n      <td>0.163413</td>\n      <td>150</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>457</th>\n      <td>746</td>\n      <td>1</td>\n      <td>0.123656</td>\n      <td>0.107174</td>\n      <td>-0.123199</td>\n      <td>0.164318</td>\n      <td>350</td>\n      <td>0.5</td>\n      <td>400</td>\n      <td>50</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>458</th>\n      <td>747</td>\n      <td>1</td>\n      <td>0.123656</td>\n      <td>0.107174</td>\n      <td>-0.123199</td>\n      <td>0.164318</td>\n      <td>350</td>\n      <td>0.0</td>\n      <td>1000</td>\n      <td>50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>459</th>\n      <td>748</td>\n      <td>1</td>\n      <td>0.123656</td>\n      <td>0.107174</td>\n      <td>-0.123199</td>\n      <td>0.164318</td>\n      <td>350</td>\n      <td>0.0</td>\n      <td>1000</td>\n      <td>50</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>460</th>\n      <td>749</td>\n      <td>1</td>\n      <td>0.123656</td>\n      <td>0.107174</td>\n      <td>-0.123199</td>\n      <td>0.164318</td>\n      <td>350</td>\n      <td>0.5</td>\n      <td>1000</td>\n      <td>50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>461</th>\n      <td>750</td>\n      <td>1</td>\n      <td>0.123656</td>\n      <td>0.107174</td>\n      <td>-0.123199</td>\n      <td>0.164318</td>\n      <td>350</td>\n      <td>0.5</td>\n      <td>1000</td>\n      <td>50</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>462 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "meta_models = meta_models.loc[:,(\"v1\",\"v2\",\"v3\",\"v4\")]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "meta_model_similarity = cosine_similarity(meta_models.values.tolist())\n",
    "meta_model_similarity = pd.DataFrame(meta_model_similarity,index=models,columns=models)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# user_similarity = cosine_similarity(data_model_train_matrix)\n",
    "# datasets_similarity = pd.DataFrame(user_similarity,index=datasets_train,columns=datasets_train)\n",
    "def cosine_similarity_func(ratings, user1, user2):\n",
    "    # 找到两个用户共同评分的物品，并将这些评分放入一个向量中\n",
    "    cols = ratings.columns[ratings.loc[user1].values.nonzero()[0]]\n",
    "    common_items = np.intersect1d(cols, ratings.columns[ratings.loc[user2].values.nonzero()[0]]).tolist()\n",
    "    u1_ratings = ratings.loc[user1,common_items]\n",
    "    u2_ratings = ratings.loc[user2,common_items]\n",
    "    # 计算两个向量之间的余弦相似度\n",
    "    if len(common_items) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        cos_sim = np.dot(u1_ratings, u2_ratings) / (np.linalg.norm(u1_ratings) * np.linalg.norm(u2_ratings))\n",
    "        return cos_sim\n",
    "\n",
    "\n",
    "dataset_similarity = pd.DataFrame(index=datasets_train, columns=datasets_train)\n",
    "for i in datasets_train:\n",
    "    for j in datasets_train:\n",
    "        dataset_similarity.loc[i][j] = cosine_similarity_func(data_model_train_matrix,i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def predict(ratings, similarity):\n",
    "#         mean_user_rating = ratings.mean(axis=1)\n",
    "#         ratings_diff = ratings - np.array(mean_user_rating)[:,np.newaxis]\n",
    "#         pred = np.array(mean_user_rating)[:,np.newaxis] + np.dot(similarity,ratings_diff) / np.array([np.abs(similarity).sum(axis = 1)]).T\n",
    "#         return pred\n",
    "\n",
    "def predict(ratings, user_similarities, user_id, item_id):\n",
    "    # 获取与用户最相似的K个用户\n",
    "    k = 1\n",
    "    similar_users = user_similarities.loc[user_id].sort_values(ascending=False).index.tolist()[1:k+1]\n",
    "    # print(similar_users)\n",
    "    # 计算加权平均评分\n",
    "    weighted_sum = 0\n",
    "    sum_of_weights = 0\n",
    "    for sim_user_id in similar_users:\n",
    "        if ratings.loc[sim_user_id][item_id] != 0:\n",
    "            # print(sim_user_id,item_id)\n",
    "            weighted_sum += user_similarities.loc[user_id][sim_user_id] * ratings.loc[sim_user_id][item_id]\n",
    "            sum_of_weights += user_similarities.loc[user_id][sim_user_id]\n",
    "    if sum_of_weights == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return weighted_sum / sum_of_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_prediction_train = pd.DataFrame(index=datasets_train,columns=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in datasets_train:\n",
    "    for j in models:\n",
    "        if data_model_train_matrix.loc[i][j] == 0:\n",
    "            model_prediction_train.loc[i][j] = predict(data_model_train_matrix,dataset_similarity,i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train_prediction = predict(data_model_train_matrix,user_similarities)\n",
    "model_prediction_train = pd.DataFrame(data_model_train_matrix,index=datasets_train,columns=models).sort_index().sort_index(axis=1)\n",
    "model_prediction_test = pd.DataFrame(index=datasets_test,columns=model_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Metadata Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_sim_index(index):\n",
    "    row1 = meta_dataset_similarity.loc[index]\n",
    "    row1_max_index = row1[row1 == row1.max()].index[0]\n",
    "    return row1_max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Find_Top_k(i,sim_matrix):\n",
    "    row = sim_matrix.loc[i]\n",
    "    row = row.sort_values(ascending=False)\n",
    "    index_row = row.index\n",
    "    index_row = index_row.values.tolist()\n",
    "    return index_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "for dataset in datasets_test:\n",
    "    for model in model_test:\n",
    "        dataset_sim_list = Find_Top_k(dataset,meta_dataset_similarity)[1:]\n",
    "        for sim_dataset in dataset_sim_list:\n",
    "            if sim_dataset not in datasets_train:\n",
    "                continue\n",
    "            if model_prediction_train.loc[sim_dataset][model] > 0.1:\n",
    "                model_prediction_test.loc[dataset][model] = model_prediction_train.loc[sim_dataset][model]\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "24.83176565170288"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time - start_time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in datasets_test:\n",
    "    for j in model_test:\n",
    "        if data_model_test_matrix.loc[i][j] == 0:\n",
    "            model_prediction_test.loc[i][j] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_result = pd.DataFrame(columns={\"dataset\",\"model\",\"predict_F1_Score\",\"groundtruth_F1_Score\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in datasets_test:\n",
    "    for j in model_test:\n",
    "        if model_prediction_test.loc[i][j] is not None:\n",
    "            predict_F1_Score = model_prediction_test.loc[i][j]\n",
    "            groundtruth_F1_Score = data_model_test_matrix.loc[i][j]\n",
    "            new_result = new_result.append([{'dataset':i,'model':j,'predict_F1_Score':predict_F1_Score,'groundtruth_F1_Score':groundtruth_F1_Score}],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "       model  dataset  groundtruth_F1_Score  predict_F1_Score\n0        289       15              0.380952          0.500000\n1        290       15              0.285714          0.181818\n2        291       15              0.615385          0.714286\n3        292       15              0.363636          0.200000\n4        293       15              0.615385          0.714286\n...      ...      ...                   ...               ...\n12539    746      284              0.023451          0.210526\n12540    747      284              0.023451          0.216216\n12541    748      284              0.023451          0.216216\n12542    749      284              0.023451          0.216216\n12543    750      284              0.023451          0.216216\n\n[12544 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>dataset</th>\n      <th>groundtruth_F1_Score</th>\n      <th>predict_F1_Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>289</td>\n      <td>15</td>\n      <td>0.380952</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>290</td>\n      <td>15</td>\n      <td>0.285714</td>\n      <td>0.181818</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>291</td>\n      <td>15</td>\n      <td>0.615385</td>\n      <td>0.714286</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>292</td>\n      <td>15</td>\n      <td>0.363636</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>293</td>\n      <td>15</td>\n      <td>0.615385</td>\n      <td>0.714286</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12539</th>\n      <td>746</td>\n      <td>284</td>\n      <td>0.023451</td>\n      <td>0.210526</td>\n    </tr>\n    <tr>\n      <th>12540</th>\n      <td>747</td>\n      <td>284</td>\n      <td>0.023451</td>\n      <td>0.216216</td>\n    </tr>\n    <tr>\n      <th>12541</th>\n      <td>748</td>\n      <td>284</td>\n      <td>0.023451</td>\n      <td>0.216216</td>\n    </tr>\n    <tr>\n      <th>12542</th>\n      <td>749</td>\n      <td>284</td>\n      <td>0.023451</td>\n      <td>0.216216</td>\n    </tr>\n    <tr>\n      <th>12543</th>\n      <td>750</td>\n      <td>284</td>\n      <td>0.023451</td>\n      <td>0.216216</td>\n    </tr>\n  </tbody>\n</table>\n<p>12544 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_result.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_result.to_csv(\"../Peak_Finding/Output/full.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "23fcb16ef9ae263cc1ee2ef7013048b59283f261690a66bd73349f654cd13bd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}