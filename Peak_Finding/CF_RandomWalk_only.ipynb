{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ratings_train = pd.read_csv(\"./Data/rate_train.csv\", low_memory=False)\n",
    "ratings_test = pd.read_csv(\"./Data/Ground_truth/groundtruth_0.3.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets_train = ratings_train.Node_Id.unique()\n",
    "model_train = ratings_train.Model_Id.unique()\n",
    "datasets_test = ratings_test.dataset.unique()\n",
    "model_test = ratings_test.model.unique()\n",
    "meta_models = pd.read_csv(\"./Data/model_v.csv\",low_memory=False)\n",
    "models = meta_models.model_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Rating Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_model_train_matrix = pd.DataFrame(index=datasets_train,columns=models)\n",
    "data_model_test_matrix = pd.DataFrame(index=datasets_test,columns=model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for row in ratings_train.itertuples():\n",
    "    data_model_train_matrix.loc[row[1]][row[2]] = row[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for row in ratings_test.itertuples():\n",
    "    data_model_test_matrix.loc[row[1]][row[2]] = row[3]\n",
    "data_model_test_matrix = data_model_test_matrix.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dataset Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "meta_datasets = pd.read_csv(\"./Data/dataset_v.csv\",low_memory=False)\n",
    "datasets = meta_datasets.dataset_id.unique()\n",
    "meta_datasets = meta_datasets.loc[:,(\"v1\",\"v2\",\"v3\",\"v4\",\"v5\")]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 对dataframe的数据进行标准化\n",
    "scaled_data = scaler.fit_transform(meta_datasets)\n",
    "# 将标准化后的数据转换为dataframe，并保留原始索引\n",
    "scaled_df = pd.DataFrame(scaled_data, index=meta_datasets.index, columns=meta_datasets.columns)\n",
    "meta_dataset_similarity = cosine_similarity(scaled_df.values.tolist())\n",
    "meta_dataset_similarity = pd.DataFrame(meta_dataset_similarity,index=datasets,columns=datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "KNN sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_func(ratings, user1, user2):\n",
    "    # 找到两个用户共同评分的物品，并将这些评分放入一个向量中\n",
    "    u1_ratings = ratings.loc[user1].dropna()\n",
    "    u2_ratings = ratings.loc[user2].dropna()\n",
    "\n",
    "    common_items = np.intersect1d(u1_ratings.index, u2_ratings.index).tolist()\n",
    "    u1_common_ratings = u1_ratings.loc[common_items]\n",
    "    u2_common_ratings = u2_ratings.loc[common_items]\n",
    "\n",
    "    # 计算两个向量之间的余弦相似度\n",
    "    if len(common_items) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        cos_sim = np.dot(u1_common_ratings, u2_common_ratings) / (np.linalg.norm(u1_common_ratings) * np.linalg.norm(u2_common_ratings))\n",
    "        return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_similarity = pd.DataFrame(index=datasets_train,columns=datasets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_bipartite_adjacency_matrix(rating_matrix):\n",
    "    n_users, n_items = rating_matrix.shape\n",
    "    adjacency_matrix = np.zeros((n_users + n_items, n_users + n_items))\n",
    "    adjacency_matrix[:n_users, n_users:] = rating_matrix\n",
    "    adjacency_matrix[n_users:, :n_users] = rating_matrix.T\n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def propagation_matrix(adjacency, lambda_):\n",
    "    n = adjacency.shape[0]\n",
    "    I = np.eye(n)\n",
    "    # 将 NaN 视为 0\n",
    "    adjacency = np.nan_to_num(adjacency)\n",
    "    try:\n",
    "        P = np.linalg.inv(I - lambda_ * adjacency)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"矩阵不可逆，无法计算传播矩阵\")\n",
    "        return None\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def propagation_matrix_withWalkLength(adjacency_matrix, max_walk_length):\n",
    "    adjacency_matrix = np.nan_to_num(adjacency_matrix)\n",
    "    propagation_matrix = np.eye(adjacency_matrix.shape[0])\n",
    "    sum_matrix = np.eye(adjacency_matrix.shape[0])\n",
    "\n",
    "    for _ in range(max_walk_length):\n",
    "        propagation_matrix = propagation_matrix @ adjacency_matrix\n",
    "        sum_matrix += propagation_matrix\n",
    "\n",
    "    return sum_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 计算二分图邻接矩阵\n",
    "bipartite_adjacency_matrix = create_bipartite_adjacency_matrix(data_model_train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "propagation_maxLength = propagation_matrix_withWalkLength(bipartite_adjacency_matrix, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 提取用户-商品传播矩阵和商品-用户传播矩阵\n",
    "n_users = data_model_train_matrix.shape[0]\n",
    "user_item_propagation = propagation_maxLength[:n_users, n_users:]\n",
    "item_user_propagation = propagation_maxLength[n_users:, :n_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 计算 Random Walk Kernel\n",
    "random_walk_kernel = np.dot(user_item_propagation, item_user_propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_kernel(kernel_matrix):\n",
    "    # 计算矩阵的最小值和最大值\n",
    "    min_val = np.min(kernel_matrix)\n",
    "    max_val = np.max(kernel_matrix)\n",
    "\n",
    "    # 防止除数为零的情况\n",
    "    if max_val == min_val:\n",
    "        return np.zeros_like(kernel_matrix)\n",
    "\n",
    "    # 将矩阵的值缩放到0和1之间\n",
    "    normalized_kernel_matrix = (kernel_matrix - min_val) / (max_val - min_val)\n",
    "\n",
    "    return normalized_kernel_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normalized_kernel = normalize_kernel(random_walk_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normalized_kernel = pd.DataFrame(normalized_kernel,index=datasets_train,columns=datasets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "          0         29        14        28        16        17        13   \\\n0    0.000965  0.091849  0.086431  0.087606  0.090945  0.051502  0.068505   \n29   0.091849  0.748755  0.709706  0.718234  0.742368  0.457248  0.580080   \n14   0.086431  0.709706  0.673637  0.681788  0.704717  0.433241  0.549737   \n28   0.087606  0.718234  0.681788  0.690040  0.713240  0.438515  0.556382   \n16   0.090945  0.742368  0.704717  0.713240  0.737206  0.453425  0.575179   \n..        ...       ...       ...       ...       ...       ...       ...   \n281  0.016304  0.202552  0.191132  0.193514  0.200355  0.119840  0.154695   \n288  0.001467  0.095459  0.089852  0.091067  0.094520  0.053731  0.071316   \n286  0.001592  0.096363  0.090715  0.091941  0.095423  0.054290  0.072021   \n287  0.001431  0.095197  0.089601  0.090814  0.094259  0.053569  0.071113   \n285  0.001310  0.094327  0.088778  0.089981  0.093399  0.053032  0.070435   \n\n          12        10        38   ...       275       274       279  \\\n0    0.060708  0.081428  0.074573  ...  0.034225  0.028086  0.019411   \n29   0.523737  0.673387  0.623937  ...  0.331599  0.287484  0.224910   \n14   0.496288  0.638295  0.591340  ...  0.313212  0.271575  0.212263   \n28   0.502303  0.645982  0.598475  ...  0.317000  0.274892  0.214887   \n16   0.519317  0.667735  0.618660  ...  0.327920  0.284419  0.222434   \n..        ...       ...       ...  ...       ...       ...       ...   \n281  0.138717  0.181179  0.167130  ...  0.084877  0.072189  0.054378   \n288  0.063252  0.084680  0.077591  ...  0.035866  0.029516  0.020544   \n286  0.063890  0.085496  0.078348  ...  0.036269  0.029869  0.020823   \n287  0.063068  0.084445  0.077372  ...  0.035751  0.029416  0.020465   \n285  0.062454  0.083660  0.076644  ...  0.035352  0.029068  0.020189   \n\n          278       280       281       288       286       287       285  \n0    0.018828  0.026387  0.016304  0.001467  0.001592  0.001431  0.001310  \n29   0.220795  0.275281  0.202552  0.095459  0.096363  0.095197  0.094327  \n14   0.208463  0.260084  0.191132  0.089852  0.090715  0.089601  0.088778  \n28   0.211049  0.263271  0.193514  0.091067  0.091941  0.090814  0.089981  \n16   0.218469  0.272414  0.200355  0.094520  0.095423  0.094259  0.093399  \n..        ...       ...       ...       ...       ...       ...       ...  \n281  0.053134  0.068663  0.047970  0.017331  0.017584  0.017259  0.017009  \n288  0.019941  0.027759  0.017331  0.001986  0.002116  0.001949  0.001824  \n286  0.020216  0.028098  0.017584  0.002116  0.002246  0.002078  0.001952  \n287  0.019862  0.027662  0.017259  0.001949  0.002078  0.001913  0.001788  \n285  0.019592  0.027329  0.017009  0.001824  0.001952  0.001788  0.001664  \n\n[249 rows x 249 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>29</th>\n      <th>14</th>\n      <th>28</th>\n      <th>16</th>\n      <th>17</th>\n      <th>13</th>\n      <th>12</th>\n      <th>10</th>\n      <th>38</th>\n      <th>...</th>\n      <th>275</th>\n      <th>274</th>\n      <th>279</th>\n      <th>278</th>\n      <th>280</th>\n      <th>281</th>\n      <th>288</th>\n      <th>286</th>\n      <th>287</th>\n      <th>285</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000965</td>\n      <td>0.091849</td>\n      <td>0.086431</td>\n      <td>0.087606</td>\n      <td>0.090945</td>\n      <td>0.051502</td>\n      <td>0.068505</td>\n      <td>0.060708</td>\n      <td>0.081428</td>\n      <td>0.074573</td>\n      <td>...</td>\n      <td>0.034225</td>\n      <td>0.028086</td>\n      <td>0.019411</td>\n      <td>0.018828</td>\n      <td>0.026387</td>\n      <td>0.016304</td>\n      <td>0.001467</td>\n      <td>0.001592</td>\n      <td>0.001431</td>\n      <td>0.001310</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.091849</td>\n      <td>0.748755</td>\n      <td>0.709706</td>\n      <td>0.718234</td>\n      <td>0.742368</td>\n      <td>0.457248</td>\n      <td>0.580080</td>\n      <td>0.523737</td>\n      <td>0.673387</td>\n      <td>0.623937</td>\n      <td>...</td>\n      <td>0.331599</td>\n      <td>0.287484</td>\n      <td>0.224910</td>\n      <td>0.220795</td>\n      <td>0.275281</td>\n      <td>0.202552</td>\n      <td>0.095459</td>\n      <td>0.096363</td>\n      <td>0.095197</td>\n      <td>0.094327</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.086431</td>\n      <td>0.709706</td>\n      <td>0.673637</td>\n      <td>0.681788</td>\n      <td>0.704717</td>\n      <td>0.433241</td>\n      <td>0.549737</td>\n      <td>0.496288</td>\n      <td>0.638295</td>\n      <td>0.591340</td>\n      <td>...</td>\n      <td>0.313212</td>\n      <td>0.271575</td>\n      <td>0.212263</td>\n      <td>0.208463</td>\n      <td>0.260084</td>\n      <td>0.191132</td>\n      <td>0.089852</td>\n      <td>0.090715</td>\n      <td>0.089601</td>\n      <td>0.088778</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.087606</td>\n      <td>0.718234</td>\n      <td>0.681788</td>\n      <td>0.690040</td>\n      <td>0.713240</td>\n      <td>0.438515</td>\n      <td>0.556382</td>\n      <td>0.502303</td>\n      <td>0.645982</td>\n      <td>0.598475</td>\n      <td>...</td>\n      <td>0.317000</td>\n      <td>0.274892</td>\n      <td>0.214887</td>\n      <td>0.211049</td>\n      <td>0.263271</td>\n      <td>0.193514</td>\n      <td>0.091067</td>\n      <td>0.091941</td>\n      <td>0.090814</td>\n      <td>0.089981</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.090945</td>\n      <td>0.742368</td>\n      <td>0.704717</td>\n      <td>0.713240</td>\n      <td>0.737206</td>\n      <td>0.453425</td>\n      <td>0.575179</td>\n      <td>0.519317</td>\n      <td>0.667735</td>\n      <td>0.618660</td>\n      <td>...</td>\n      <td>0.327920</td>\n      <td>0.284419</td>\n      <td>0.222434</td>\n      <td>0.218469</td>\n      <td>0.272414</td>\n      <td>0.200355</td>\n      <td>0.094520</td>\n      <td>0.095423</td>\n      <td>0.094259</td>\n      <td>0.093399</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>281</th>\n      <td>0.016304</td>\n      <td>0.202552</td>\n      <td>0.191132</td>\n      <td>0.193514</td>\n      <td>0.200355</td>\n      <td>0.119840</td>\n      <td>0.154695</td>\n      <td>0.138717</td>\n      <td>0.181179</td>\n      <td>0.167130</td>\n      <td>...</td>\n      <td>0.084877</td>\n      <td>0.072189</td>\n      <td>0.054378</td>\n      <td>0.053134</td>\n      <td>0.068663</td>\n      <td>0.047970</td>\n      <td>0.017331</td>\n      <td>0.017584</td>\n      <td>0.017259</td>\n      <td>0.017009</td>\n    </tr>\n    <tr>\n      <th>288</th>\n      <td>0.001467</td>\n      <td>0.095459</td>\n      <td>0.089852</td>\n      <td>0.091067</td>\n      <td>0.094520</td>\n      <td>0.053731</td>\n      <td>0.071316</td>\n      <td>0.063252</td>\n      <td>0.084680</td>\n      <td>0.077591</td>\n      <td>...</td>\n      <td>0.035866</td>\n      <td>0.029516</td>\n      <td>0.020544</td>\n      <td>0.019941</td>\n      <td>0.027759</td>\n      <td>0.017331</td>\n      <td>0.001986</td>\n      <td>0.002116</td>\n      <td>0.001949</td>\n      <td>0.001824</td>\n    </tr>\n    <tr>\n      <th>286</th>\n      <td>0.001592</td>\n      <td>0.096363</td>\n      <td>0.090715</td>\n      <td>0.091941</td>\n      <td>0.095423</td>\n      <td>0.054290</td>\n      <td>0.072021</td>\n      <td>0.063890</td>\n      <td>0.085496</td>\n      <td>0.078348</td>\n      <td>...</td>\n      <td>0.036269</td>\n      <td>0.029869</td>\n      <td>0.020823</td>\n      <td>0.020216</td>\n      <td>0.028098</td>\n      <td>0.017584</td>\n      <td>0.002116</td>\n      <td>0.002246</td>\n      <td>0.002078</td>\n      <td>0.001952</td>\n    </tr>\n    <tr>\n      <th>287</th>\n      <td>0.001431</td>\n      <td>0.095197</td>\n      <td>0.089601</td>\n      <td>0.090814</td>\n      <td>0.094259</td>\n      <td>0.053569</td>\n      <td>0.071113</td>\n      <td>0.063068</td>\n      <td>0.084445</td>\n      <td>0.077372</td>\n      <td>...</td>\n      <td>0.035751</td>\n      <td>0.029416</td>\n      <td>0.020465</td>\n      <td>0.019862</td>\n      <td>0.027662</td>\n      <td>0.017259</td>\n      <td>0.001949</td>\n      <td>0.002078</td>\n      <td>0.001913</td>\n      <td>0.001788</td>\n    </tr>\n    <tr>\n      <th>285</th>\n      <td>0.001310</td>\n      <td>0.094327</td>\n      <td>0.088778</td>\n      <td>0.089981</td>\n      <td>0.093399</td>\n      <td>0.053032</td>\n      <td>0.070435</td>\n      <td>0.062454</td>\n      <td>0.083660</td>\n      <td>0.076644</td>\n      <td>...</td>\n      <td>0.035352</td>\n      <td>0.029068</td>\n      <td>0.020189</td>\n      <td>0.019592</td>\n      <td>0.027329</td>\n      <td>0.017009</td>\n      <td>0.001824</td>\n      <td>0.001952</td>\n      <td>0.001788</td>\n      <td>0.001664</td>\n    </tr>\n  </tbody>\n</table>\n<p>249 rows × 249 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_kernel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lambda_ = 0.5\n",
    "for i in datasets_train:\n",
    "    for j in datasets_train:\n",
    "        if normalized_kernel.loc[i][j] != 0:\n",
    "            dataset_similarity.loc[i][j] = normalized_kernel.loc[i][j]\n",
    "        else:\n",
    "            dataset_similarity.loc[i][j] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          0         29        14        28        16        17        13   \\\n0    0.000965  0.091849  0.086431  0.087606  0.090945  0.051502  0.068505   \n29   0.091849  0.748755  0.709706  0.718234  0.742368  0.457248   0.58008   \n14   0.086431  0.709706  0.673637  0.681788  0.704717  0.433241  0.549737   \n28   0.087606  0.718234  0.681788   0.69004   0.71324  0.438515  0.556382   \n16   0.090945  0.742368  0.704717   0.71324  0.737206  0.453425  0.575179   \n..        ...       ...       ...       ...       ...       ...       ...   \n281  0.016304  0.202552  0.191132  0.193514  0.200355   0.11984  0.154695   \n288  0.001467  0.095459  0.089852  0.091067   0.09452  0.053731  0.071316   \n286  0.001592  0.096363  0.090715  0.091941  0.095423   0.05429  0.072021   \n287  0.001431  0.095197  0.089601  0.090814  0.094259  0.053569  0.071113   \n285   0.00131  0.094327  0.088778  0.089981  0.093399  0.053032  0.070435   \n\n          12        10        38   ...       275       274       279  \\\n0    0.060708  0.081428  0.074573  ...  0.034225  0.028086  0.019411   \n29   0.523737  0.673387  0.623937  ...  0.331599  0.287484   0.22491   \n14   0.496288  0.638295   0.59134  ...  0.313212  0.271575  0.212263   \n28   0.502303  0.645982  0.598475  ...     0.317  0.274892  0.214887   \n16   0.519317  0.667735   0.61866  ...   0.32792  0.284419  0.222434   \n..        ...       ...       ...  ...       ...       ...       ...   \n281  0.138717  0.181179   0.16713  ...  0.084877  0.072189  0.054378   \n288  0.063252   0.08468  0.077591  ...  0.035866  0.029516  0.020544   \n286   0.06389  0.085496  0.078348  ...  0.036269  0.029869  0.020823   \n287  0.063068  0.084445  0.077372  ...  0.035751  0.029416  0.020465   \n285  0.062454   0.08366  0.076644  ...  0.035352  0.029068  0.020189   \n\n          278       280       281       288       286       287       285  \n0    0.018828  0.026387  0.016304  0.001467  0.001592  0.001431   0.00131  \n29   0.220795  0.275281  0.202552  0.095459  0.096363  0.095197  0.094327  \n14   0.208463  0.260084  0.191132  0.089852  0.090715  0.089601  0.088778  \n28   0.211049  0.263271  0.193514  0.091067  0.091941  0.090814  0.089981  \n16   0.218469  0.272414  0.200355   0.09452  0.095423  0.094259  0.093399  \n..        ...       ...       ...       ...       ...       ...       ...  \n281  0.053134  0.068663   0.04797  0.017331  0.017584  0.017259  0.017009  \n288  0.019941  0.027759  0.017331  0.001986  0.002116  0.001949  0.001824  \n286  0.020216  0.028098  0.017584  0.002116  0.002246  0.002078  0.001952  \n287  0.019862  0.027662  0.017259  0.001949  0.002078  0.001913  0.001788  \n285  0.019592  0.027329  0.017009  0.001824  0.001952  0.001788  0.001664  \n\n[249 rows x 249 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>29</th>\n      <th>14</th>\n      <th>28</th>\n      <th>16</th>\n      <th>17</th>\n      <th>13</th>\n      <th>12</th>\n      <th>10</th>\n      <th>38</th>\n      <th>...</th>\n      <th>275</th>\n      <th>274</th>\n      <th>279</th>\n      <th>278</th>\n      <th>280</th>\n      <th>281</th>\n      <th>288</th>\n      <th>286</th>\n      <th>287</th>\n      <th>285</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000965</td>\n      <td>0.091849</td>\n      <td>0.086431</td>\n      <td>0.087606</td>\n      <td>0.090945</td>\n      <td>0.051502</td>\n      <td>0.068505</td>\n      <td>0.060708</td>\n      <td>0.081428</td>\n      <td>0.074573</td>\n      <td>...</td>\n      <td>0.034225</td>\n      <td>0.028086</td>\n      <td>0.019411</td>\n      <td>0.018828</td>\n      <td>0.026387</td>\n      <td>0.016304</td>\n      <td>0.001467</td>\n      <td>0.001592</td>\n      <td>0.001431</td>\n      <td>0.00131</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.091849</td>\n      <td>0.748755</td>\n      <td>0.709706</td>\n      <td>0.718234</td>\n      <td>0.742368</td>\n      <td>0.457248</td>\n      <td>0.58008</td>\n      <td>0.523737</td>\n      <td>0.673387</td>\n      <td>0.623937</td>\n      <td>...</td>\n      <td>0.331599</td>\n      <td>0.287484</td>\n      <td>0.22491</td>\n      <td>0.220795</td>\n      <td>0.275281</td>\n      <td>0.202552</td>\n      <td>0.095459</td>\n      <td>0.096363</td>\n      <td>0.095197</td>\n      <td>0.094327</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.086431</td>\n      <td>0.709706</td>\n      <td>0.673637</td>\n      <td>0.681788</td>\n      <td>0.704717</td>\n      <td>0.433241</td>\n      <td>0.549737</td>\n      <td>0.496288</td>\n      <td>0.638295</td>\n      <td>0.59134</td>\n      <td>...</td>\n      <td>0.313212</td>\n      <td>0.271575</td>\n      <td>0.212263</td>\n      <td>0.208463</td>\n      <td>0.260084</td>\n      <td>0.191132</td>\n      <td>0.089852</td>\n      <td>0.090715</td>\n      <td>0.089601</td>\n      <td>0.088778</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.087606</td>\n      <td>0.718234</td>\n      <td>0.681788</td>\n      <td>0.69004</td>\n      <td>0.71324</td>\n      <td>0.438515</td>\n      <td>0.556382</td>\n      <td>0.502303</td>\n      <td>0.645982</td>\n      <td>0.598475</td>\n      <td>...</td>\n      <td>0.317</td>\n      <td>0.274892</td>\n      <td>0.214887</td>\n      <td>0.211049</td>\n      <td>0.263271</td>\n      <td>0.193514</td>\n      <td>0.091067</td>\n      <td>0.091941</td>\n      <td>0.090814</td>\n      <td>0.089981</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.090945</td>\n      <td>0.742368</td>\n      <td>0.704717</td>\n      <td>0.71324</td>\n      <td>0.737206</td>\n      <td>0.453425</td>\n      <td>0.575179</td>\n      <td>0.519317</td>\n      <td>0.667735</td>\n      <td>0.61866</td>\n      <td>...</td>\n      <td>0.32792</td>\n      <td>0.284419</td>\n      <td>0.222434</td>\n      <td>0.218469</td>\n      <td>0.272414</td>\n      <td>0.200355</td>\n      <td>0.09452</td>\n      <td>0.095423</td>\n      <td>0.094259</td>\n      <td>0.093399</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>281</th>\n      <td>0.016304</td>\n      <td>0.202552</td>\n      <td>0.191132</td>\n      <td>0.193514</td>\n      <td>0.200355</td>\n      <td>0.11984</td>\n      <td>0.154695</td>\n      <td>0.138717</td>\n      <td>0.181179</td>\n      <td>0.16713</td>\n      <td>...</td>\n      <td>0.084877</td>\n      <td>0.072189</td>\n      <td>0.054378</td>\n      <td>0.053134</td>\n      <td>0.068663</td>\n      <td>0.04797</td>\n      <td>0.017331</td>\n      <td>0.017584</td>\n      <td>0.017259</td>\n      <td>0.017009</td>\n    </tr>\n    <tr>\n      <th>288</th>\n      <td>0.001467</td>\n      <td>0.095459</td>\n      <td>0.089852</td>\n      <td>0.091067</td>\n      <td>0.09452</td>\n      <td>0.053731</td>\n      <td>0.071316</td>\n      <td>0.063252</td>\n      <td>0.08468</td>\n      <td>0.077591</td>\n      <td>...</td>\n      <td>0.035866</td>\n      <td>0.029516</td>\n      <td>0.020544</td>\n      <td>0.019941</td>\n      <td>0.027759</td>\n      <td>0.017331</td>\n      <td>0.001986</td>\n      <td>0.002116</td>\n      <td>0.001949</td>\n      <td>0.001824</td>\n    </tr>\n    <tr>\n      <th>286</th>\n      <td>0.001592</td>\n      <td>0.096363</td>\n      <td>0.090715</td>\n      <td>0.091941</td>\n      <td>0.095423</td>\n      <td>0.05429</td>\n      <td>0.072021</td>\n      <td>0.06389</td>\n      <td>0.085496</td>\n      <td>0.078348</td>\n      <td>...</td>\n      <td>0.036269</td>\n      <td>0.029869</td>\n      <td>0.020823</td>\n      <td>0.020216</td>\n      <td>0.028098</td>\n      <td>0.017584</td>\n      <td>0.002116</td>\n      <td>0.002246</td>\n      <td>0.002078</td>\n      <td>0.001952</td>\n    </tr>\n    <tr>\n      <th>287</th>\n      <td>0.001431</td>\n      <td>0.095197</td>\n      <td>0.089601</td>\n      <td>0.090814</td>\n      <td>0.094259</td>\n      <td>0.053569</td>\n      <td>0.071113</td>\n      <td>0.063068</td>\n      <td>0.084445</td>\n      <td>0.077372</td>\n      <td>...</td>\n      <td>0.035751</td>\n      <td>0.029416</td>\n      <td>0.020465</td>\n      <td>0.019862</td>\n      <td>0.027662</td>\n      <td>0.017259</td>\n      <td>0.001949</td>\n      <td>0.002078</td>\n      <td>0.001913</td>\n      <td>0.001788</td>\n    </tr>\n    <tr>\n      <th>285</th>\n      <td>0.00131</td>\n      <td>0.094327</td>\n      <td>0.088778</td>\n      <td>0.089981</td>\n      <td>0.093399</td>\n      <td>0.053032</td>\n      <td>0.070435</td>\n      <td>0.062454</td>\n      <td>0.08366</td>\n      <td>0.076644</td>\n      <td>...</td>\n      <td>0.035352</td>\n      <td>0.029068</td>\n      <td>0.020189</td>\n      <td>0.019592</td>\n      <td>0.027329</td>\n      <td>0.017009</td>\n      <td>0.001824</td>\n      <td>0.001952</td>\n      <td>0.001788</td>\n      <td>0.001664</td>\n    </tr>\n  </tbody>\n</table>\n<p>249 rows × 249 columns</p>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_ratings(rating_matrix, user_similarity_matrix, k=5):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "    rating_matrix - 评分矩阵，DataFrame格式，其中NaN表示未评分\n",
    "    user_similarity_matrix - 用户相似度矩阵，DataFrame格式\n",
    "    k - 最近邻的数量，默认为5\n",
    "\n",
    "    输出：\n",
    "    prediction_matrix - 预测矩阵，DataFrame格式\n",
    "    \"\"\"\n",
    "\n",
    "    # 初始化预测矩阵\n",
    "    prediction_matrix = rating_matrix.copy()\n",
    "\n",
    "    # 对于评分矩阵中的每个NaN值，使用K最近邻的方法预测评分\n",
    "    for i in rating_matrix.index:\n",
    "        for j in rating_matrix.columns:\n",
    "            if np.isnan(rating_matrix.loc[i][j]):\n",
    "                # 获取第i个用户的相似度值，并在相似度矩阵中找到K个最相似的用户\n",
    "                similarity_values = user_similarity_matrix.loc[i].sort_values(ascending=False)[1:k+1]\n",
    "\n",
    "                # 计算加权平均评分\n",
    "                weighted_sum = 0\n",
    "                similarity_sum = 0\n",
    "                for index, value in similarity_values.items():\n",
    "                    user_rating = rating_matrix.loc[index][j]\n",
    "                    if not np.isnan(user_rating):\n",
    "                        weighted_sum += value * user_rating\n",
    "                        similarity_sum += value\n",
    "\n",
    "                # 如果存在至少一个相似用户对该物品进行了评分，则计算预测评分\n",
    "                if similarity_sum != 0:\n",
    "                    prediction_matrix.loc[i][j] = weighted_sum / similarity_sum\n",
    "                else:\n",
    "                    # 如果没有相似用户评分，则使用当前用户的平均评分作为预测值\n",
    "                    prediction_matrix.loc[i][j] = rating_matrix[j].mean()\n",
    "\n",
    "    return prediction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def predict(rating_matrix, similarity_matrix):\n",
    "    \"\"\"\n",
    "    根据评分矩阵和相似度矩阵预测评分。\n",
    "\n",
    "    参数：\n",
    "    rating_matrix (pd.DataFrame)：评分矩阵，包含NaN值\n",
    "    similarity_matrix (numpy.array)：相似度矩阵\n",
    "\n",
    "    返回：\n",
    "    pd.DataFrame：预测评分矩阵\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取评分矩阵的均值（忽略NaN值）\n",
    "    mean_rating = rating_matrix.mean(axis=1).values\n",
    "\n",
    "    # 将评分矩阵中的NaN值替换为0\n",
    "    rating_matrix_nan_to_zero = rating_matrix.fillna(0).values\n",
    "\n",
    "    # 减去均值，得到归一化的评分矩阵\n",
    "    normalized_rating_matrix = rating_matrix_nan_to_zero - mean_rating[:, np.newaxis]\n",
    "\n",
    "    # 计算预测评分\n",
    "    predicted_ratings = mean_rating[:, np.newaxis] + np.dot(similarity_matrix, normalized_rating_matrix) / np.abs(similarity_matrix).sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # 将预测评分数组转换为DataFrame\n",
    "    predicted_ratings_df = pd.DataFrame(predicted_ratings, index=rating_matrix.index, columns=rating_matrix.columns)\n",
    "\n",
    "    return predicted_ratings_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\byy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:23: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n"
     ]
    }
   ],
   "source": [
    "model_prediction_train = predict(data_model_train_matrix,dataset_similarity)\n",
    "model_prediction_train = pd.DataFrame(model_prediction_train,index=datasets_train,columns=models).sort_index().sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          289       290       291       292       293       294       295  \\\n0    -0.13011 -0.237457 -0.048142 -0.225821 -0.056037 -0.225902 -0.057897   \n1    0.274531  0.167565  0.353542  0.178953  0.345768  0.178853  0.343946   \n2    0.485845  0.378678  0.564846  0.390074  0.557062  0.389973  0.555237   \n3    0.490984  0.383832  0.569976  0.395226  0.562193  0.395125  0.560369   \n4    0.565081  0.457919  0.644032  0.469311   0.63625   0.46921  0.634426   \n..        ...       ...       ...       ...       ...       ...       ...   \n281 -0.027985  -0.13455  0.051778 -0.123123  0.043999 -0.123217  0.042172   \n285 -0.128121 -0.235442 -0.046255 -0.223815 -0.054145 -0.223897 -0.056004   \n286 -0.126907 -0.234229 -0.045111 -0.222607 -0.052999  -0.22269 -0.054857   \n287 -0.127202 -0.234509 -0.045374 -0.222885 -0.053262 -0.222967 -0.055121   \n288 -0.127758 -0.235071 -0.045936 -0.223447 -0.053824  -0.22353 -0.055682   \n\n          296       297       298  ...       741       742       743  \\\n0   -0.226034 -0.057537 -0.226034  ... -0.531556 -0.531558 -0.531552   \n1     0.17865  0.344264   0.17865  ... -0.119012 -0.119031 -0.119015   \n2    0.389767  0.555554  0.389767  ...  0.092371  0.092352  0.092368   \n3     0.39492  0.560685   0.39492  ...  0.097532  0.097513  0.097529   \n4    0.469003  0.634742  0.469003  ...  0.171741  0.171722  0.171738   \n..        ...       ...       ...  ...       ...       ...       ...   \n281 -0.123396  0.042502 -0.123396  ... -0.423613 -0.423626 -0.423613   \n285 -0.224031 -0.055646 -0.224031  ... -0.529295 -0.529298 -0.529292   \n286 -0.222825 -0.054499 -0.222825  ... -0.527895 -0.527898 -0.527891   \n287 -0.223102 -0.054763 -0.223102  ... -0.528278 -0.528281 -0.528274   \n288 -0.223665 -0.055325 -0.223665  ... -0.528817  -0.52882 -0.528814   \n\n          744       745       746       747       748       749       750  \n0   -0.531675 -0.531673 -0.531675 -0.531639 -0.531641 -0.531639 -0.531641  \n1   -0.119218   -0.1192 -0.119218 -0.119213 -0.119225 -0.119213 -0.119225  \n2    0.092161   0.09218  0.092161  0.092165  0.092153  0.092165  0.092153  \n3    0.097322  0.097341  0.097322  0.097326  0.097314  0.097326  0.097314  \n4     0.17153  0.171549   0.17153  0.171534  0.171521  0.171534  0.171521  \n..        ...       ...       ...       ...       ...       ...       ...  \n281  -0.42379 -0.423777  -0.42379 -0.423774 -0.423782 -0.423774 -0.423782  \n285 -0.529417 -0.529414 -0.529417 -0.529382 -0.529384 -0.529382 -0.529384  \n286 -0.528018 -0.528015 -0.528018 -0.527984 -0.527986 -0.527984 -0.527986  \n287   -0.5284 -0.528398   -0.5284 -0.528366 -0.528368 -0.528366 -0.528368  \n288  -0.52894 -0.528937  -0.52894 -0.528906 -0.528908 -0.528906 -0.528908  \n\n[249 rows x 462 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>289</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>...</th>\n      <th>741</th>\n      <th>742</th>\n      <th>743</th>\n      <th>744</th>\n      <th>745</th>\n      <th>746</th>\n      <th>747</th>\n      <th>748</th>\n      <th>749</th>\n      <th>750</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.13011</td>\n      <td>-0.237457</td>\n      <td>-0.048142</td>\n      <td>-0.225821</td>\n      <td>-0.056037</td>\n      <td>-0.225902</td>\n      <td>-0.057897</td>\n      <td>-0.226034</td>\n      <td>-0.057537</td>\n      <td>-0.226034</td>\n      <td>...</td>\n      <td>-0.531556</td>\n      <td>-0.531558</td>\n      <td>-0.531552</td>\n      <td>-0.531675</td>\n      <td>-0.531673</td>\n      <td>-0.531675</td>\n      <td>-0.531639</td>\n      <td>-0.531641</td>\n      <td>-0.531639</td>\n      <td>-0.531641</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.274531</td>\n      <td>0.167565</td>\n      <td>0.353542</td>\n      <td>0.178953</td>\n      <td>0.345768</td>\n      <td>0.178853</td>\n      <td>0.343946</td>\n      <td>0.17865</td>\n      <td>0.344264</td>\n      <td>0.17865</td>\n      <td>...</td>\n      <td>-0.119012</td>\n      <td>-0.119031</td>\n      <td>-0.119015</td>\n      <td>-0.119218</td>\n      <td>-0.1192</td>\n      <td>-0.119218</td>\n      <td>-0.119213</td>\n      <td>-0.119225</td>\n      <td>-0.119213</td>\n      <td>-0.119225</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.485845</td>\n      <td>0.378678</td>\n      <td>0.564846</td>\n      <td>0.390074</td>\n      <td>0.557062</td>\n      <td>0.389973</td>\n      <td>0.555237</td>\n      <td>0.389767</td>\n      <td>0.555554</td>\n      <td>0.389767</td>\n      <td>...</td>\n      <td>0.092371</td>\n      <td>0.092352</td>\n      <td>0.092368</td>\n      <td>0.092161</td>\n      <td>0.09218</td>\n      <td>0.092161</td>\n      <td>0.092165</td>\n      <td>0.092153</td>\n      <td>0.092165</td>\n      <td>0.092153</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.490984</td>\n      <td>0.383832</td>\n      <td>0.569976</td>\n      <td>0.395226</td>\n      <td>0.562193</td>\n      <td>0.395125</td>\n      <td>0.560369</td>\n      <td>0.39492</td>\n      <td>0.560685</td>\n      <td>0.39492</td>\n      <td>...</td>\n      <td>0.097532</td>\n      <td>0.097513</td>\n      <td>0.097529</td>\n      <td>0.097322</td>\n      <td>0.097341</td>\n      <td>0.097322</td>\n      <td>0.097326</td>\n      <td>0.097314</td>\n      <td>0.097326</td>\n      <td>0.097314</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.565081</td>\n      <td>0.457919</td>\n      <td>0.644032</td>\n      <td>0.469311</td>\n      <td>0.63625</td>\n      <td>0.46921</td>\n      <td>0.634426</td>\n      <td>0.469003</td>\n      <td>0.634742</td>\n      <td>0.469003</td>\n      <td>...</td>\n      <td>0.171741</td>\n      <td>0.171722</td>\n      <td>0.171738</td>\n      <td>0.17153</td>\n      <td>0.171549</td>\n      <td>0.17153</td>\n      <td>0.171534</td>\n      <td>0.171521</td>\n      <td>0.171534</td>\n      <td>0.171521</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>281</th>\n      <td>-0.027985</td>\n      <td>-0.13455</td>\n      <td>0.051778</td>\n      <td>-0.123123</td>\n      <td>0.043999</td>\n      <td>-0.123217</td>\n      <td>0.042172</td>\n      <td>-0.123396</td>\n      <td>0.042502</td>\n      <td>-0.123396</td>\n      <td>...</td>\n      <td>-0.423613</td>\n      <td>-0.423626</td>\n      <td>-0.423613</td>\n      <td>-0.42379</td>\n      <td>-0.423777</td>\n      <td>-0.42379</td>\n      <td>-0.423774</td>\n      <td>-0.423782</td>\n      <td>-0.423774</td>\n      <td>-0.423782</td>\n    </tr>\n    <tr>\n      <th>285</th>\n      <td>-0.128121</td>\n      <td>-0.235442</td>\n      <td>-0.046255</td>\n      <td>-0.223815</td>\n      <td>-0.054145</td>\n      <td>-0.223897</td>\n      <td>-0.056004</td>\n      <td>-0.224031</td>\n      <td>-0.055646</td>\n      <td>-0.224031</td>\n      <td>...</td>\n      <td>-0.529295</td>\n      <td>-0.529298</td>\n      <td>-0.529292</td>\n      <td>-0.529417</td>\n      <td>-0.529414</td>\n      <td>-0.529417</td>\n      <td>-0.529382</td>\n      <td>-0.529384</td>\n      <td>-0.529382</td>\n      <td>-0.529384</td>\n    </tr>\n    <tr>\n      <th>286</th>\n      <td>-0.126907</td>\n      <td>-0.234229</td>\n      <td>-0.045111</td>\n      <td>-0.222607</td>\n      <td>-0.052999</td>\n      <td>-0.22269</td>\n      <td>-0.054857</td>\n      <td>-0.222825</td>\n      <td>-0.054499</td>\n      <td>-0.222825</td>\n      <td>...</td>\n      <td>-0.527895</td>\n      <td>-0.527898</td>\n      <td>-0.527891</td>\n      <td>-0.528018</td>\n      <td>-0.528015</td>\n      <td>-0.528018</td>\n      <td>-0.527984</td>\n      <td>-0.527986</td>\n      <td>-0.527984</td>\n      <td>-0.527986</td>\n    </tr>\n    <tr>\n      <th>287</th>\n      <td>-0.127202</td>\n      <td>-0.234509</td>\n      <td>-0.045374</td>\n      <td>-0.222885</td>\n      <td>-0.053262</td>\n      <td>-0.222967</td>\n      <td>-0.055121</td>\n      <td>-0.223102</td>\n      <td>-0.054763</td>\n      <td>-0.223102</td>\n      <td>...</td>\n      <td>-0.528278</td>\n      <td>-0.528281</td>\n      <td>-0.528274</td>\n      <td>-0.5284</td>\n      <td>-0.528398</td>\n      <td>-0.5284</td>\n      <td>-0.528366</td>\n      <td>-0.528368</td>\n      <td>-0.528366</td>\n      <td>-0.528368</td>\n    </tr>\n    <tr>\n      <th>288</th>\n      <td>-0.127758</td>\n      <td>-0.235071</td>\n      <td>-0.045936</td>\n      <td>-0.223447</td>\n      <td>-0.053824</td>\n      <td>-0.22353</td>\n      <td>-0.055682</td>\n      <td>-0.223665</td>\n      <td>-0.055325</td>\n      <td>-0.223665</td>\n      <td>...</td>\n      <td>-0.528817</td>\n      <td>-0.52882</td>\n      <td>-0.528814</td>\n      <td>-0.52894</td>\n      <td>-0.528937</td>\n      <td>-0.52894</td>\n      <td>-0.528906</td>\n      <td>-0.528908</td>\n      <td>-0.528906</td>\n      <td>-0.528908</td>\n    </tr>\n  </tbody>\n</table>\n<p>249 rows × 462 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_prediction_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_prediction_test = pd.DataFrame(index=datasets_test,columns=model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "8.288299560546875"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time_train = time.time()\n",
    "Training_time = end_time_train - start_time_train\n",
    "Training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_sim_index(index):\n",
    "    row1 = meta_dataset_similarity.loc[index]\n",
    "    row1_max_index = row1[row1 == row1.max()].index[0]\n",
    "    return row1_max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Find_Top_k(i,sim_matrix):\n",
    "    row = sim_matrix.loc[i]\n",
    "    row = row.sort_values(ascending=False)\n",
    "    index_row = row.index\n",
    "    index_row = index_row.values.tolist()\n",
    "    return index_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for dataset in datasets_test:\n",
    "    for model in model_test:\n",
    "        dataset_sim_list = Find_Top_k(dataset,meta_dataset_similarity)[1:]\n",
    "        # 仅保留存在于 model_prediction_train 的索引\n",
    "        valid_indices = [idx for idx in dataset_sim_list if idx in model_prediction_train.index][:15]\n",
    "        model_prediction_test.loc[dataset][model] = model_prediction_train.loc[valid_indices][model].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[271, 270, 269, 281, 280, 266, 265, 264, 263, 262, 255, 256, 268, 276, 279]"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "10.040815591812134"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in datasets_test:\n",
    "    for j in model_test:\n",
    "        if data_model_test_matrix.loc[i][j] == 0:\n",
    "            model_prediction_test.loc[i][j] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns={\"dataset\",\"model\",\"f1_score\",\"groundtruth_f1_score\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in datasets_test:\n",
    "    for j in model_test:\n",
    "        if model_prediction_test.loc[i][j] is not None:\n",
    "            f1_score = model_prediction_test.loc[i][j]\n",
    "            groundtruth_f1_score = data_model_test_matrix.loc[i][j]\n",
    "            result = result.append([{'dataset':i,'model':j,'f1_score':f1_score,'groundtruth_f1_score':groundtruth_f1_score}],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     dataset  groundtruth_f1_score model  f1_score\n0         93              0.300000   463  0.381069\n1         93              0.300000   465  0.375416\n2         93              0.909091   363  0.591702\n3         93              0.303030   367  0.528832\n4         93              0.303030   369  0.517794\n...      ...                   ...   ...       ...\n8207     283              0.908689   307 -0.283689\n8208     283              0.882246   317  0.433330\n8209     283              0.885276   315  0.458599\n8210     283              0.751981   303 -0.480643\n8211     283              0.752451   305 -0.480211\n\n[8212 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>groundtruth_f1_score</th>\n      <th>model</th>\n      <th>f1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>93</td>\n      <td>0.300000</td>\n      <td>463</td>\n      <td>0.381069</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>93</td>\n      <td>0.300000</td>\n      <td>465</td>\n      <td>0.375416</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>93</td>\n      <td>0.909091</td>\n      <td>363</td>\n      <td>0.591702</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>93</td>\n      <td>0.303030</td>\n      <td>367</td>\n      <td>0.528832</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>93</td>\n      <td>0.303030</td>\n      <td>369</td>\n      <td>0.517794</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8207</th>\n      <td>283</td>\n      <td>0.908689</td>\n      <td>307</td>\n      <td>-0.283689</td>\n    </tr>\n    <tr>\n      <th>8208</th>\n      <td>283</td>\n      <td>0.882246</td>\n      <td>317</td>\n      <td>0.433330</td>\n    </tr>\n    <tr>\n      <th>8209</th>\n      <td>283</td>\n      <td>0.885276</td>\n      <td>315</td>\n      <td>0.458599</td>\n    </tr>\n    <tr>\n      <th>8210</th>\n      <td>283</td>\n      <td>0.751981</td>\n      <td>303</td>\n      <td>-0.480643</td>\n    </tr>\n    <tr>\n      <th>8211</th>\n      <td>283</td>\n      <td>0.752451</td>\n      <td>305</td>\n      <td>-0.480211</td>\n    </tr>\n  </tbody>\n</table>\n<p>8212 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result.to_csv(\"../Peak_Finding/Output/RandomWalk_only/Full_RandomWalk@4@15.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "23fcb16ef9ae263cc1ee2ef7013048b59283f261690a66bd73349f654cd13bd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}