{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import random\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ratings= pd.read_csv(\"./Data/ratings.csv\", low_memory=False)\n",
    "train_data = pd.read_csv(\"./Data/train_data.csv\",low_memory=False)\n",
    "test_data = pd.read_csv(\"./Data/test_data.csv\",low_memory=False)\n",
    "# train_data, test_data = train_test_split(ratings, test_size=0.3, random_state=42)\n",
    "# # 保存训练集为csv文件\n",
    "# train_data.to_csv('./Data/train_data.csv',index=False)\n",
    "# # 保存测试集为csv文件\n",
    "# test_data.to_csv('./Data/test_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets = ratings.dataset_id.unique()\n",
    "models = ratings.model_id.unique()\n",
    "datasets_train = train_data.dataset_id.unique()\n",
    "model_train = train_data.model_id.unique()\n",
    "datasets_test = test_data.dataset_id.unique()\n",
    "model_test = test_data.model_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Rating Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_model_train_matrix = pd.DataFrame(index=datasets_train,columns=model_train)\n",
    "data_model_test_matrix = pd.DataFrame(index=datasets_test,columns=model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for row in train_data.itertuples():\n",
    "    data_model_train_matrix.loc[row[1]][row[2]] = row[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for row in test_data.itertuples():\n",
    "    data_model_test_matrix.loc[row[1]][row[2]] = row[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dataset Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "outputs": [],
   "source": [
    "def create_graph(df):\n",
    "    G = nx.Graph()\n",
    "    for _, group in df.groupby('model_id'):\n",
    "        users = group['dataset_id'].values\n",
    "        ratings = group['F1_Score'].values\n",
    "        for i in range(len(users)):\n",
    "            for j in range(i + 1, len(users)):\n",
    "                user1, user2 = users[i], users[j]\n",
    "                rating = (ratings[i] + ratings[j])/2.0  # 使用评分的平均值作为边的权重\n",
    "                if G.has_edge(user1, user2):\n",
    "                    G[user1][user2]['sum_rating'] += rating\n",
    "                    G[user1][user2]['count'] += 1\n",
    "                else:\n",
    "                    G.add_edge(user1, user2, sum_rating=rating, count=1)\n",
    "    return G"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "outputs": [],
   "source": [
    "def random_walk(G, num_walks, walk_length):\n",
    "    walks = []\n",
    "    for _ in range(num_walks):\n",
    "        for node in G.nodes():\n",
    "            walk = [node]\n",
    "            for _ in range(walk_length):\n",
    "                neighbors = list(G.neighbors(walk[-1]))\n",
    "                if len(neighbors) > 0:\n",
    "                    next_node = random.choices(\n",
    "                        neighbors,\n",
    "                        weights=[G[walk[-1]][neighbor]['sum_rating'] * G[walk[-1]][neighbor]['count'] for neighbor in neighbors],\n",
    "                        k=1\n",
    "                    )[0]\n",
    "                    walk.append(next_node)\n",
    "            walks.append(walk)\n",
    "    return walks\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "outputs": [],
   "source": [
    "def compute_similarity(walks, G, num_walks):\n",
    "    user_to_index = {user: i for i, user in enumerate(G.nodes)}\n",
    "    similarity = np.zeros((len(G), len(G)))\n",
    "    interactions = np.zeros((len(G), len(G)))\n",
    "\n",
    "    for walk in walks:\n",
    "        for i in range(len(walk)):\n",
    "            for j in range(i+1, len(walk)):\n",
    "                user1, user2 = walk[i], walk[j]\n",
    "                index1, index2 = user_to_index[user1], user_to_index[user2]\n",
    "                # 检查用户1和用户2之间是否存在边\n",
    "                if G.has_edge(user1, user2):\n",
    "                    similarity[index1][index2] += G[user1][user2]['sum_rating'] / G[user1][user2]['count']\n",
    "                    similarity[index2][index1] += G[user1][user2]['sum_rating'] / G[user1][user2]['count']\n",
    "                    interactions[index1][index2] += 1\n",
    "                    interactions[index2][index1] += 1\n",
    "\n",
    "    # 在实际计算相似度之前，避免除以0\n",
    "    interactions[interactions == 0] = 1\n",
    "    similarity /= interactions\n",
    "\n",
    "    # 把相同的用户相似度设为1\n",
    "    np.fill_diagonal(similarity, 1)\n",
    "\n",
    "    similarity_df = pd.DataFrame(similarity, index=G.nodes, columns=G.nodes)\n",
    "    return similarity_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "outputs": [],
   "source": [
    "\n",
    "# 创建图\n",
    "G = create_graph(train_data)\n",
    "\n",
    "# 进行随机游走\n",
    "walks = random_walk(G, num_walks=100, walk_length=4)\n",
    "\n",
    "# 计算相似性\n",
    "similarity_df = compute_similarity(walks, G,num_walks=200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "outputs": [],
   "source": [
    "similarity_df = pd.DataFrame(similarity_df,index=data_model_train_matrix.index,columns=data_model_train_matrix.index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "outputs": [
    {
     "data": {
      "text/plain": "          48        14        60        237       250       246       51   \\\n48   1.000000  0.635264  0.636880  0.341744  0.361661  0.359926  0.542870   \n14   0.635264  1.000000  0.635528  0.378529  0.384368  0.368701  0.583242   \n60   0.636880  0.635528  1.000000  0.348588  0.373837  0.367432  0.554001   \n237  0.341744  0.378529  0.348588  1.000000  0.066482  0.059305  0.276896   \n250  0.361661  0.384368  0.373837  0.066482  1.000000  0.075872  0.288840   \n..        ...       ...       ...       ...       ...       ...       ...   \n205  0.695585  0.691358  0.685016  0.421793  0.433982  0.436410  0.609613   \n71   0.644005  0.658879  0.646487  0.388934  0.385779  0.386977  0.561437   \n109  0.627996  0.622222  0.622676  0.342566  0.353086  0.359799  0.537026   \n59   0.631138  0.640432  0.620878  0.369407  0.372591  0.365034  0.555804   \n76   0.679075  0.701114  0.670870  0.406976  0.419774  0.424448  0.612177   \n\n          1         74        248  ...       50        149       85   \\\n48   0.552943  0.575515  0.369860  ...  0.591221  0.661203  0.694131   \n14   0.574539  0.614149  0.385136  ...  0.611635  0.682211  0.000000   \n60   0.550004  0.582733  0.373396  ...  0.596101  0.660887  0.690425   \n237  0.275333  0.281924  0.059511  ...  0.308660  0.411174  0.410384   \n250  0.282753  0.298686  0.075939  ...  0.319468  0.416367  0.432042   \n..        ...       ...       ...  ...       ...       ...       ...   \n205  0.630260  0.631856  0.437738  ...  0.643257  0.716119  0.756004   \n71   0.576818  0.596447  0.382713  ...  0.609979  0.679478  0.721446   \n109  0.553989  0.576227  0.364982  ...  0.583429  0.648286  0.680536   \n59   0.557551  0.571115  0.370066  ...  0.593869  0.670920  0.693471   \n76   0.617322  0.626709  0.422118  ...  0.637783  0.729106  0.751716   \n\n          128       229       205       71        109       59        76   \n48   0.587014  0.656844  0.695585  0.644005  0.627996  0.631138  0.679075  \n14   0.623094  0.661252  0.691358  0.658879  0.622222  0.640432  0.701114  \n60   0.594583  0.653685  0.685016  0.646487  0.622676  0.620878  0.670870  \n237  0.311459  0.387088  0.421793  0.388934  0.342566  0.369407  0.406976  \n250  0.321879  0.395456  0.433982  0.385779  0.353086  0.372591  0.419774  \n..        ...       ...       ...       ...       ...       ...       ...  \n205  0.647838  0.709163  1.000000  0.698446  0.679396  0.705742  0.738042  \n71   0.608434  0.668721  0.698446  1.000000  0.635598  0.650454  0.704002  \n109  0.584950  0.646207  0.679396  0.635598  1.000000  0.610169  0.664326  \n59   0.592145  0.659606  0.705742  0.650454  0.610169  1.000000  0.693634  \n76   0.643920  0.713761  0.738042  0.704002  0.664326  0.693634  1.000000  \n\n[289 rows x 289 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>48</th>\n      <th>14</th>\n      <th>60</th>\n      <th>237</th>\n      <th>250</th>\n      <th>246</th>\n      <th>51</th>\n      <th>1</th>\n      <th>74</th>\n      <th>248</th>\n      <th>...</th>\n      <th>50</th>\n      <th>149</th>\n      <th>85</th>\n      <th>128</th>\n      <th>229</th>\n      <th>205</th>\n      <th>71</th>\n      <th>109</th>\n      <th>59</th>\n      <th>76</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>48</th>\n      <td>1.000000</td>\n      <td>0.635264</td>\n      <td>0.636880</td>\n      <td>0.341744</td>\n      <td>0.361661</td>\n      <td>0.359926</td>\n      <td>0.542870</td>\n      <td>0.552943</td>\n      <td>0.575515</td>\n      <td>0.369860</td>\n      <td>...</td>\n      <td>0.591221</td>\n      <td>0.661203</td>\n      <td>0.694131</td>\n      <td>0.587014</td>\n      <td>0.656844</td>\n      <td>0.695585</td>\n      <td>0.644005</td>\n      <td>0.627996</td>\n      <td>0.631138</td>\n      <td>0.679075</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.635264</td>\n      <td>1.000000</td>\n      <td>0.635528</td>\n      <td>0.378529</td>\n      <td>0.384368</td>\n      <td>0.368701</td>\n      <td>0.583242</td>\n      <td>0.574539</td>\n      <td>0.614149</td>\n      <td>0.385136</td>\n      <td>...</td>\n      <td>0.611635</td>\n      <td>0.682211</td>\n      <td>0.000000</td>\n      <td>0.623094</td>\n      <td>0.661252</td>\n      <td>0.691358</td>\n      <td>0.658879</td>\n      <td>0.622222</td>\n      <td>0.640432</td>\n      <td>0.701114</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>0.636880</td>\n      <td>0.635528</td>\n      <td>1.000000</td>\n      <td>0.348588</td>\n      <td>0.373837</td>\n      <td>0.367432</td>\n      <td>0.554001</td>\n      <td>0.550004</td>\n      <td>0.582733</td>\n      <td>0.373396</td>\n      <td>...</td>\n      <td>0.596101</td>\n      <td>0.660887</td>\n      <td>0.690425</td>\n      <td>0.594583</td>\n      <td>0.653685</td>\n      <td>0.685016</td>\n      <td>0.646487</td>\n      <td>0.622676</td>\n      <td>0.620878</td>\n      <td>0.670870</td>\n    </tr>\n    <tr>\n      <th>237</th>\n      <td>0.341744</td>\n      <td>0.378529</td>\n      <td>0.348588</td>\n      <td>1.000000</td>\n      <td>0.066482</td>\n      <td>0.059305</td>\n      <td>0.276896</td>\n      <td>0.275333</td>\n      <td>0.281924</td>\n      <td>0.059511</td>\n      <td>...</td>\n      <td>0.308660</td>\n      <td>0.411174</td>\n      <td>0.410384</td>\n      <td>0.311459</td>\n      <td>0.387088</td>\n      <td>0.421793</td>\n      <td>0.388934</td>\n      <td>0.342566</td>\n      <td>0.369407</td>\n      <td>0.406976</td>\n    </tr>\n    <tr>\n      <th>250</th>\n      <td>0.361661</td>\n      <td>0.384368</td>\n      <td>0.373837</td>\n      <td>0.066482</td>\n      <td>1.000000</td>\n      <td>0.075872</td>\n      <td>0.288840</td>\n      <td>0.282753</td>\n      <td>0.298686</td>\n      <td>0.075939</td>\n      <td>...</td>\n      <td>0.319468</td>\n      <td>0.416367</td>\n      <td>0.432042</td>\n      <td>0.321879</td>\n      <td>0.395456</td>\n      <td>0.433982</td>\n      <td>0.385779</td>\n      <td>0.353086</td>\n      <td>0.372591</td>\n      <td>0.419774</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>0.695585</td>\n      <td>0.691358</td>\n      <td>0.685016</td>\n      <td>0.421793</td>\n      <td>0.433982</td>\n      <td>0.436410</td>\n      <td>0.609613</td>\n      <td>0.630260</td>\n      <td>0.631856</td>\n      <td>0.437738</td>\n      <td>...</td>\n      <td>0.643257</td>\n      <td>0.716119</td>\n      <td>0.756004</td>\n      <td>0.647838</td>\n      <td>0.709163</td>\n      <td>1.000000</td>\n      <td>0.698446</td>\n      <td>0.679396</td>\n      <td>0.705742</td>\n      <td>0.738042</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>0.644005</td>\n      <td>0.658879</td>\n      <td>0.646487</td>\n      <td>0.388934</td>\n      <td>0.385779</td>\n      <td>0.386977</td>\n      <td>0.561437</td>\n      <td>0.576818</td>\n      <td>0.596447</td>\n      <td>0.382713</td>\n      <td>...</td>\n      <td>0.609979</td>\n      <td>0.679478</td>\n      <td>0.721446</td>\n      <td>0.608434</td>\n      <td>0.668721</td>\n      <td>0.698446</td>\n      <td>1.000000</td>\n      <td>0.635598</td>\n      <td>0.650454</td>\n      <td>0.704002</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>0.627996</td>\n      <td>0.622222</td>\n      <td>0.622676</td>\n      <td>0.342566</td>\n      <td>0.353086</td>\n      <td>0.359799</td>\n      <td>0.537026</td>\n      <td>0.553989</td>\n      <td>0.576227</td>\n      <td>0.364982</td>\n      <td>...</td>\n      <td>0.583429</td>\n      <td>0.648286</td>\n      <td>0.680536</td>\n      <td>0.584950</td>\n      <td>0.646207</td>\n      <td>0.679396</td>\n      <td>0.635598</td>\n      <td>1.000000</td>\n      <td>0.610169</td>\n      <td>0.664326</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>0.631138</td>\n      <td>0.640432</td>\n      <td>0.620878</td>\n      <td>0.369407</td>\n      <td>0.372591</td>\n      <td>0.365034</td>\n      <td>0.555804</td>\n      <td>0.557551</td>\n      <td>0.571115</td>\n      <td>0.370066</td>\n      <td>...</td>\n      <td>0.593869</td>\n      <td>0.670920</td>\n      <td>0.693471</td>\n      <td>0.592145</td>\n      <td>0.659606</td>\n      <td>0.705742</td>\n      <td>0.650454</td>\n      <td>0.610169</td>\n      <td>1.000000</td>\n      <td>0.693634</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>0.679075</td>\n      <td>0.701114</td>\n      <td>0.670870</td>\n      <td>0.406976</td>\n      <td>0.419774</td>\n      <td>0.424448</td>\n      <td>0.612177</td>\n      <td>0.617322</td>\n      <td>0.626709</td>\n      <td>0.422118</td>\n      <td>...</td>\n      <td>0.637783</td>\n      <td>0.729106</td>\n      <td>0.751716</td>\n      <td>0.643920</td>\n      <td>0.713761</td>\n      <td>0.738042</td>\n      <td>0.704002</td>\n      <td>0.664326</td>\n      <td>0.693634</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>289 rows × 289 columns</p>\n</div>"
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "outputs": [],
   "source": [
    "dataset_similarity = pd.DataFrame(index=datasets_train,columns=datasets_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "outputs": [],
   "source": [
    "for i in datasets_train:\n",
    "    for j in datasets_train:\n",
    "        dataset_similarity.loc[i][j] = similarity_df.loc[i][j]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "outputs": [
    {
     "data": {
      "text/plain": "          48        14        60        237       250       246       51   \\\n48        1.0  0.635264   0.63688  0.341744  0.361661  0.359926   0.54287   \n14   0.635264       1.0  0.635528  0.378529  0.384368  0.368701  0.583242   \n60    0.63688  0.635528       1.0  0.348588  0.373837  0.367432  0.554001   \n237  0.341744  0.378529  0.348588       1.0  0.066482  0.059305  0.276896   \n250  0.361661  0.384368  0.373837  0.066482       1.0  0.075872   0.28884   \n..        ...       ...       ...       ...       ...       ...       ...   \n205  0.695585  0.691358  0.685016  0.421793  0.433982   0.43641  0.609613   \n71   0.644005  0.658879  0.646487  0.388934  0.385779  0.386977  0.561437   \n109  0.627996  0.622222  0.622676  0.342566  0.353086  0.359799  0.537026   \n59   0.631138  0.640432  0.620878  0.369407  0.372591  0.365034  0.555804   \n76   0.679075  0.701114   0.67087  0.406976  0.419774  0.424448  0.612177   \n\n          1         74        248  ...       50        149       85   \\\n48   0.552943  0.575515   0.36986  ...  0.591221  0.661203  0.694131   \n14   0.574539  0.614149  0.385136  ...  0.611635  0.682211       0.0   \n60   0.550004  0.582733  0.373396  ...  0.596101  0.660887  0.690425   \n237  0.275333  0.281924  0.059511  ...   0.30866  0.411174  0.410384   \n250  0.282753  0.298686  0.075939  ...  0.319468  0.416367  0.432042   \n..        ...       ...       ...  ...       ...       ...       ...   \n205   0.63026  0.631856  0.437738  ...  0.643257  0.716119  0.756004   \n71   0.576818  0.596447  0.382713  ...  0.609979  0.679478  0.721446   \n109  0.553989  0.576227  0.364982  ...  0.583429  0.648286  0.680536   \n59   0.557551  0.571115  0.370066  ...  0.593869   0.67092  0.693471   \n76   0.617322  0.626709  0.422118  ...  0.637783  0.729106  0.751716   \n\n          128       229       205       71        109       59        76   \n48   0.587014  0.656844  0.695585  0.644005  0.627996  0.631138  0.679075  \n14   0.623094  0.661252  0.691358  0.658879  0.622222  0.640432  0.701114  \n60   0.594583  0.653685  0.685016  0.646487  0.622676  0.620878   0.67087  \n237  0.311459  0.387088  0.421793  0.388934  0.342566  0.369407  0.406976  \n250  0.321879  0.395456  0.433982  0.385779  0.353086  0.372591  0.419774  \n..        ...       ...       ...       ...       ...       ...       ...  \n205  0.647838  0.709163       1.0  0.698446  0.679396  0.705742  0.738042  \n71   0.608434  0.668721  0.698446       1.0  0.635598  0.650454  0.704002  \n109   0.58495  0.646207  0.679396  0.635598       1.0  0.610169  0.664326  \n59   0.592145  0.659606  0.705742  0.650454  0.610169       1.0  0.693634  \n76    0.64392  0.713761  0.738042  0.704002  0.664326  0.693634       1.0  \n\n[289 rows x 289 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>48</th>\n      <th>14</th>\n      <th>60</th>\n      <th>237</th>\n      <th>250</th>\n      <th>246</th>\n      <th>51</th>\n      <th>1</th>\n      <th>74</th>\n      <th>248</th>\n      <th>...</th>\n      <th>50</th>\n      <th>149</th>\n      <th>85</th>\n      <th>128</th>\n      <th>229</th>\n      <th>205</th>\n      <th>71</th>\n      <th>109</th>\n      <th>59</th>\n      <th>76</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>48</th>\n      <td>1.0</td>\n      <td>0.635264</td>\n      <td>0.63688</td>\n      <td>0.341744</td>\n      <td>0.361661</td>\n      <td>0.359926</td>\n      <td>0.54287</td>\n      <td>0.552943</td>\n      <td>0.575515</td>\n      <td>0.36986</td>\n      <td>...</td>\n      <td>0.591221</td>\n      <td>0.661203</td>\n      <td>0.694131</td>\n      <td>0.587014</td>\n      <td>0.656844</td>\n      <td>0.695585</td>\n      <td>0.644005</td>\n      <td>0.627996</td>\n      <td>0.631138</td>\n      <td>0.679075</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.635264</td>\n      <td>1.0</td>\n      <td>0.635528</td>\n      <td>0.378529</td>\n      <td>0.384368</td>\n      <td>0.368701</td>\n      <td>0.583242</td>\n      <td>0.574539</td>\n      <td>0.614149</td>\n      <td>0.385136</td>\n      <td>...</td>\n      <td>0.611635</td>\n      <td>0.682211</td>\n      <td>0.0</td>\n      <td>0.623094</td>\n      <td>0.661252</td>\n      <td>0.691358</td>\n      <td>0.658879</td>\n      <td>0.622222</td>\n      <td>0.640432</td>\n      <td>0.701114</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>0.63688</td>\n      <td>0.635528</td>\n      <td>1.0</td>\n      <td>0.348588</td>\n      <td>0.373837</td>\n      <td>0.367432</td>\n      <td>0.554001</td>\n      <td>0.550004</td>\n      <td>0.582733</td>\n      <td>0.373396</td>\n      <td>...</td>\n      <td>0.596101</td>\n      <td>0.660887</td>\n      <td>0.690425</td>\n      <td>0.594583</td>\n      <td>0.653685</td>\n      <td>0.685016</td>\n      <td>0.646487</td>\n      <td>0.622676</td>\n      <td>0.620878</td>\n      <td>0.67087</td>\n    </tr>\n    <tr>\n      <th>237</th>\n      <td>0.341744</td>\n      <td>0.378529</td>\n      <td>0.348588</td>\n      <td>1.0</td>\n      <td>0.066482</td>\n      <td>0.059305</td>\n      <td>0.276896</td>\n      <td>0.275333</td>\n      <td>0.281924</td>\n      <td>0.059511</td>\n      <td>...</td>\n      <td>0.30866</td>\n      <td>0.411174</td>\n      <td>0.410384</td>\n      <td>0.311459</td>\n      <td>0.387088</td>\n      <td>0.421793</td>\n      <td>0.388934</td>\n      <td>0.342566</td>\n      <td>0.369407</td>\n      <td>0.406976</td>\n    </tr>\n    <tr>\n      <th>250</th>\n      <td>0.361661</td>\n      <td>0.384368</td>\n      <td>0.373837</td>\n      <td>0.066482</td>\n      <td>1.0</td>\n      <td>0.075872</td>\n      <td>0.28884</td>\n      <td>0.282753</td>\n      <td>0.298686</td>\n      <td>0.075939</td>\n      <td>...</td>\n      <td>0.319468</td>\n      <td>0.416367</td>\n      <td>0.432042</td>\n      <td>0.321879</td>\n      <td>0.395456</td>\n      <td>0.433982</td>\n      <td>0.385779</td>\n      <td>0.353086</td>\n      <td>0.372591</td>\n      <td>0.419774</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>0.695585</td>\n      <td>0.691358</td>\n      <td>0.685016</td>\n      <td>0.421793</td>\n      <td>0.433982</td>\n      <td>0.43641</td>\n      <td>0.609613</td>\n      <td>0.63026</td>\n      <td>0.631856</td>\n      <td>0.437738</td>\n      <td>...</td>\n      <td>0.643257</td>\n      <td>0.716119</td>\n      <td>0.756004</td>\n      <td>0.647838</td>\n      <td>0.709163</td>\n      <td>1.0</td>\n      <td>0.698446</td>\n      <td>0.679396</td>\n      <td>0.705742</td>\n      <td>0.738042</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>0.644005</td>\n      <td>0.658879</td>\n      <td>0.646487</td>\n      <td>0.388934</td>\n      <td>0.385779</td>\n      <td>0.386977</td>\n      <td>0.561437</td>\n      <td>0.576818</td>\n      <td>0.596447</td>\n      <td>0.382713</td>\n      <td>...</td>\n      <td>0.609979</td>\n      <td>0.679478</td>\n      <td>0.721446</td>\n      <td>0.608434</td>\n      <td>0.668721</td>\n      <td>0.698446</td>\n      <td>1.0</td>\n      <td>0.635598</td>\n      <td>0.650454</td>\n      <td>0.704002</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>0.627996</td>\n      <td>0.622222</td>\n      <td>0.622676</td>\n      <td>0.342566</td>\n      <td>0.353086</td>\n      <td>0.359799</td>\n      <td>0.537026</td>\n      <td>0.553989</td>\n      <td>0.576227</td>\n      <td>0.364982</td>\n      <td>...</td>\n      <td>0.583429</td>\n      <td>0.648286</td>\n      <td>0.680536</td>\n      <td>0.58495</td>\n      <td>0.646207</td>\n      <td>0.679396</td>\n      <td>0.635598</td>\n      <td>1.0</td>\n      <td>0.610169</td>\n      <td>0.664326</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>0.631138</td>\n      <td>0.640432</td>\n      <td>0.620878</td>\n      <td>0.369407</td>\n      <td>0.372591</td>\n      <td>0.365034</td>\n      <td>0.555804</td>\n      <td>0.557551</td>\n      <td>0.571115</td>\n      <td>0.370066</td>\n      <td>...</td>\n      <td>0.593869</td>\n      <td>0.67092</td>\n      <td>0.693471</td>\n      <td>0.592145</td>\n      <td>0.659606</td>\n      <td>0.705742</td>\n      <td>0.650454</td>\n      <td>0.610169</td>\n      <td>1.0</td>\n      <td>0.693634</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>0.679075</td>\n      <td>0.701114</td>\n      <td>0.67087</td>\n      <td>0.406976</td>\n      <td>0.419774</td>\n      <td>0.424448</td>\n      <td>0.612177</td>\n      <td>0.617322</td>\n      <td>0.626709</td>\n      <td>0.422118</td>\n      <td>...</td>\n      <td>0.637783</td>\n      <td>0.729106</td>\n      <td>0.751716</td>\n      <td>0.64392</td>\n      <td>0.713761</td>\n      <td>0.738042</td>\n      <td>0.704002</td>\n      <td>0.664326</td>\n      <td>0.693634</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>289 rows × 289 columns</p>\n</div>"
     },
     "execution_count": 761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_similarity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "outputs": [],
   "source": [
    "start_time_train = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "end_time_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.01529073715209961"
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time_train - start_time_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time_ref = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 预测函数\n",
    "def predict(ratings, similarity):\n",
    "    mean_user_rating = ratings.fillna(0).mean(axis=1)\n",
    "    ratings_diff = (ratings - mean_user_rating[:, np.newaxis]).fillna(0)\n",
    "    pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "     # 只替换NaN值\n",
    "    df_nan = ratings.isnull()\n",
    "    pred = pd.DataFrame(pred).where(df_nan, ratings)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\byy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\byy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "user_prediction = predict(data_model_train_matrix,dataset_similarity).sort_index(axis=0).sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "end_time_ref = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "4.550584554672241"
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time_ref - start_time_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mask = (data_model_test_matrix.fillna(0) != 0) & (user_prediction != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "outputs": [],
   "source": [
    "# 只选择那些在预测评分和实际评分中都不是 0 的评分\n",
    "prediction = user_prediction[mask].values.flatten()\n",
    "prediction = pd.to_numeric(prediction, errors='coerce')\n",
    "prediction = prediction[~np.isnan(prediction)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "actual = data_model_test_matrix.fillna(0)[mask].values.flatten()\n",
    "actual = pd.to_numeric(actual, errors='coerce')\n",
    "actual = actual[~np.isnan(actual)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_rmse(prediction, actual):\n",
    "    # 计算 RMSE\n",
    "    return sqrt(mean_squared_error(prediction, actual))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_rmse = calculate_rmse(prediction, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.36561230348957907"
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ndcg(y_true, y_pred, k):\n",
    "    \"\"\"计算 NDCG @k\n",
    "    y_true: 真实的 relevancy 分数（通常为 0 或 1）\n",
    "    y_pred: 预测的 relevancy 分数\n",
    "    k: 截断位置\n",
    "    \"\"\"\n",
    "    # 计算 DCG @k\n",
    "    order = np.argsort(y_pred)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    dcg = np.sum(gains / discounts)\n",
    "\n",
    "    # 计算 IDCG @k\n",
    "    ideal_order = np.argsort(y_true)[::-1]\n",
    "    ideal_gains = 2 ** np.take(y_true, ideal_order[:k]) - 1\n",
    "    ideal_discounts = np.log2(np.arange(len(ideal_gains)) + 2)\n",
    "    idcg = np.sum(ideal_gains / ideal_discounts)\n",
    "\n",
    "    # 防止0除问题\n",
    "    if idcg == 0:\n",
    "        return 0\n",
    "\n",
    "    # 计算 NDCG @k\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.8361291886983206"
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg(actual, prediction, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "23fcb16ef9ae263cc1ee2ef7013048b59283f261690a66bd73349f654cd13bd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}