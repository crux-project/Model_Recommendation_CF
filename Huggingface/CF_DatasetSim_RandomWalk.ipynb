{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ratings_train = pd.read_csv(\"./Data/rate_train.csv\", low_memory=False)\n",
    "ratings_test = pd.read_csv(\"./Data/rate_test.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets_train = ratings_train.dataset_id.unique()\n",
    "model_train = ratings_train.model_id.unique()\n",
    "datasets_test = ratings_test.dataset_id.unique()\n",
    "model_test = ratings_test.model_id.unique()\n",
    "meta_models = pd.read_csv(\"./Data/models_v.csv\",low_memory=False)\n",
    "models = meta_models.model_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Rating Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_model_train_matrix = pd.DataFrame(index=datasets_train,columns=models)\n",
    "data_model_test_matrix = pd.DataFrame(index=datasets_test,columns=model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for row in ratings_train.itertuples():\n",
    "    data_model_train_matrix.loc[row[1]][row[2]] = row[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for row in ratings_test.itertuples():\n",
    "    data_model_test_matrix.loc[row[1]][row[2]] = row[3]\n",
    "data_model_test_matrix = data_model_test_matrix.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dataset Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "meta_datasets = pd.read_csv(\"./Data/dataset_v.csv\",low_memory=False)\n",
    "datasets = meta_datasets.dataset_id.unique()\n",
    "meta_datasets = meta_datasets.loc[:,(\"v1\",\"v2\",\"v3\",\"v4\",\"v5\",\"v6\",\"v7\",\"v8\",\"v9\",\"v10\",\"v11\",\"v12\",\"v13\",\"v14\",\"v15\",\"v16\")]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 对dataframe的数据进行标准化\n",
    "scaled_data = scaler.fit_transform(meta_datasets)\n",
    "# 将标准化后的数据转换为dataframe，并保留原始索引\n",
    "scaled_df = pd.DataFrame(scaled_data, index=meta_datasets.index, columns=meta_datasets.columns)\n",
    "meta_dataset_similarity = cosine_similarity(scaled_df.values.tolist())\n",
    "meta_dataset_similarity = pd.DataFrame(meta_dataset_similarity,index=datasets,columns=datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "KNN sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          v1        v2        v3        v4        v5        v6        v7  \\\n0   0.025034  0.003197  0.013935 -0.013116  0.012825 -0.020438 -0.014044   \n1   0.037312  0.012165  0.021228 -0.026170  0.013586 -0.025984 -0.016653   \n2   0.017211  0.004628  0.007155 -0.011390  0.008059 -0.013901 -0.005064   \n3   0.025696  0.008171  0.015039 -0.025142  0.009067 -0.032023 -0.015498   \n4   0.027573  0.010065  0.014708 -0.018399  0.013069 -0.028164 -0.011714   \n..       ...       ...       ...       ...       ...       ...       ...   \n61  0.019118  0.006924  0.013132 -0.017057  0.008796 -0.021800 -0.014749   \n62  0.037312  0.010336  0.014128 -0.018743  0.015076 -0.037564 -0.015761   \n63  0.026009  0.010832  0.017085 -0.015846  0.015780 -0.027117 -0.016596   \n64  0.025716  0.011888  0.016567 -0.018135  0.021027 -0.036057 -0.018491   \n65  0.035484  0.004584  0.018729 -0.008592  0.018021 -0.034186 -0.017310   \n\n          v8        v9       v10       v11       v12       v13       v14  \\\n0   0.003616  0.006384  0.003538 -0.023130  0.006932  0.004364 -0.004096   \n1  -0.002769  0.013702  0.008140 -0.036311  0.008131  0.014083 -0.005162   \n2  -0.003218 -0.001625  0.000959 -0.010429 -0.004005  0.008344 -0.008625   \n3  -0.004472  0.005749  0.008614 -0.029290  0.012309  0.011675 -0.003869   \n4   0.002462  0.007283  0.002274 -0.026866  0.006382  0.007245 -0.003877   \n..       ...       ...       ...       ...       ...       ...       ...   \n61  0.006227  0.003878  0.005254 -0.030610  0.001404  0.006432 -0.007262   \n62  0.014260  0.008174  0.004831 -0.034659  0.005804  0.005876 -0.009112   \n63  0.003597  0.005324  0.002113 -0.026415  0.007599  0.009518 -0.007011   \n64  0.002428  0.002532  0.000715 -0.029196  0.006693  0.005483 -0.007920   \n65  0.006231  0.004916 -0.002483 -0.030811  0.002624  0.003750 -0.008838   \n\n         v15       v16  \n0  -0.003078  0.004269  \n1   0.003845  0.009943  \n2   0.002013  0.000405  \n3  -0.002635 -0.003028  \n4  -0.002586  0.005021  \n..       ...       ...  \n61 -0.002656  0.002973  \n62  0.001246  0.013720  \n63 -0.004881  0.006763  \n64 -0.005947  0.005251  \n65 -0.004513  0.015991  \n\n[66 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>v3</th>\n      <th>v4</th>\n      <th>v5</th>\n      <th>v6</th>\n      <th>v7</th>\n      <th>v8</th>\n      <th>v9</th>\n      <th>v10</th>\n      <th>v11</th>\n      <th>v12</th>\n      <th>v13</th>\n      <th>v14</th>\n      <th>v15</th>\n      <th>v16</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.025034</td>\n      <td>0.003197</td>\n      <td>0.013935</td>\n      <td>-0.013116</td>\n      <td>0.012825</td>\n      <td>-0.020438</td>\n      <td>-0.014044</td>\n      <td>0.003616</td>\n      <td>0.006384</td>\n      <td>0.003538</td>\n      <td>-0.023130</td>\n      <td>0.006932</td>\n      <td>0.004364</td>\n      <td>-0.004096</td>\n      <td>-0.003078</td>\n      <td>0.004269</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.037312</td>\n      <td>0.012165</td>\n      <td>0.021228</td>\n      <td>-0.026170</td>\n      <td>0.013586</td>\n      <td>-0.025984</td>\n      <td>-0.016653</td>\n      <td>-0.002769</td>\n      <td>0.013702</td>\n      <td>0.008140</td>\n      <td>-0.036311</td>\n      <td>0.008131</td>\n      <td>0.014083</td>\n      <td>-0.005162</td>\n      <td>0.003845</td>\n      <td>0.009943</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.017211</td>\n      <td>0.004628</td>\n      <td>0.007155</td>\n      <td>-0.011390</td>\n      <td>0.008059</td>\n      <td>-0.013901</td>\n      <td>-0.005064</td>\n      <td>-0.003218</td>\n      <td>-0.001625</td>\n      <td>0.000959</td>\n      <td>-0.010429</td>\n      <td>-0.004005</td>\n      <td>0.008344</td>\n      <td>-0.008625</td>\n      <td>0.002013</td>\n      <td>0.000405</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.025696</td>\n      <td>0.008171</td>\n      <td>0.015039</td>\n      <td>-0.025142</td>\n      <td>0.009067</td>\n      <td>-0.032023</td>\n      <td>-0.015498</td>\n      <td>-0.004472</td>\n      <td>0.005749</td>\n      <td>0.008614</td>\n      <td>-0.029290</td>\n      <td>0.012309</td>\n      <td>0.011675</td>\n      <td>-0.003869</td>\n      <td>-0.002635</td>\n      <td>-0.003028</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.027573</td>\n      <td>0.010065</td>\n      <td>0.014708</td>\n      <td>-0.018399</td>\n      <td>0.013069</td>\n      <td>-0.028164</td>\n      <td>-0.011714</td>\n      <td>0.002462</td>\n      <td>0.007283</td>\n      <td>0.002274</td>\n      <td>-0.026866</td>\n      <td>0.006382</td>\n      <td>0.007245</td>\n      <td>-0.003877</td>\n      <td>-0.002586</td>\n      <td>0.005021</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>0.019118</td>\n      <td>0.006924</td>\n      <td>0.013132</td>\n      <td>-0.017057</td>\n      <td>0.008796</td>\n      <td>-0.021800</td>\n      <td>-0.014749</td>\n      <td>0.006227</td>\n      <td>0.003878</td>\n      <td>0.005254</td>\n      <td>-0.030610</td>\n      <td>0.001404</td>\n      <td>0.006432</td>\n      <td>-0.007262</td>\n      <td>-0.002656</td>\n      <td>0.002973</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>0.037312</td>\n      <td>0.010336</td>\n      <td>0.014128</td>\n      <td>-0.018743</td>\n      <td>0.015076</td>\n      <td>-0.037564</td>\n      <td>-0.015761</td>\n      <td>0.014260</td>\n      <td>0.008174</td>\n      <td>0.004831</td>\n      <td>-0.034659</td>\n      <td>0.005804</td>\n      <td>0.005876</td>\n      <td>-0.009112</td>\n      <td>0.001246</td>\n      <td>0.013720</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>0.026009</td>\n      <td>0.010832</td>\n      <td>0.017085</td>\n      <td>-0.015846</td>\n      <td>0.015780</td>\n      <td>-0.027117</td>\n      <td>-0.016596</td>\n      <td>0.003597</td>\n      <td>0.005324</td>\n      <td>0.002113</td>\n      <td>-0.026415</td>\n      <td>0.007599</td>\n      <td>0.009518</td>\n      <td>-0.007011</td>\n      <td>-0.004881</td>\n      <td>0.006763</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>0.025716</td>\n      <td>0.011888</td>\n      <td>0.016567</td>\n      <td>-0.018135</td>\n      <td>0.021027</td>\n      <td>-0.036057</td>\n      <td>-0.018491</td>\n      <td>0.002428</td>\n      <td>0.002532</td>\n      <td>0.000715</td>\n      <td>-0.029196</td>\n      <td>0.006693</td>\n      <td>0.005483</td>\n      <td>-0.007920</td>\n      <td>-0.005947</td>\n      <td>0.005251</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>0.035484</td>\n      <td>0.004584</td>\n      <td>0.018729</td>\n      <td>-0.008592</td>\n      <td>0.018021</td>\n      <td>-0.034186</td>\n      <td>-0.017310</td>\n      <td>0.006231</td>\n      <td>0.004916</td>\n      <td>-0.002483</td>\n      <td>-0.030811</td>\n      <td>0.002624</td>\n      <td>0.003750</td>\n      <td>-0.008838</td>\n      <td>-0.004513</td>\n      <td>0.015991</td>\n    </tr>\n  </tbody>\n</table>\n<p>66 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "def cosine_similarity_func(ratings, user1, user2):\n",
    "    # 找到两个用户共同评分的物品，并将这些评分放入一个向量中\n",
    "    u1_ratings = ratings.loc[user1].dropna()\n",
    "    u2_ratings = ratings.loc[user2].dropna()\n",
    "\n",
    "    common_items = np.intersect1d(u1_ratings.index, u2_ratings.index).tolist()\n",
    "    u1_common_ratings = u1_ratings.loc[common_items]\n",
    "    u2_common_ratings = u2_ratings.loc[common_items]\n",
    "\n",
    "    # 计算两个向量之间的余弦相似度\n",
    "    if len(common_items) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        cos_sim = np.dot(u1_common_ratings, u2_common_ratings) / (np.linalg.norm(u1_common_ratings) * np.linalg.norm(u2_common_ratings))\n",
    "        return cos_sim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "dataset_similarity = pd.DataFrame(index=datasets_train,columns=datasets_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "def create_bipartite_adjacency_matrix(rating_matrix):\n",
    "    n_users, n_items = rating_matrix.shape\n",
    "    adjacency_matrix = np.zeros((n_users + n_items, n_users + n_items))\n",
    "    adjacency_matrix[:n_users, n_users:] = rating_matrix\n",
    "    adjacency_matrix[n_users:, :n_users] = rating_matrix.T\n",
    "    return adjacency_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "def propagation_matrix(adjacency, lambda_):\n",
    "    n = adjacency.shape[0]\n",
    "    I = np.eye(n)\n",
    "    # 将 NaN 视为 0\n",
    "    adjacency = np.nan_to_num(adjacency)\n",
    "    try:\n",
    "        P = np.linalg.inv(I - lambda_ * adjacency)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"矩阵不可逆，无法计算传播矩阵\")\n",
    "        return None\n",
    "    return P"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "def propagation_matrix_withWalkLength(adjacency_matrix, max_walk_length):\n",
    "    adjacency_matrix = np.nan_to_num(adjacency_matrix)\n",
    "    propagation_matrix = np.eye(adjacency_matrix.shape[0])\n",
    "    sum_matrix = np.eye(adjacency_matrix.shape[0])\n",
    "\n",
    "    for _ in range(max_walk_length):\n",
    "        propagation_matrix = propagation_matrix @ adjacency_matrix\n",
    "        sum_matrix += propagation_matrix\n",
    "\n",
    "    return sum_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "start_time_train = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "# 计算二分图邻接矩阵\n",
    "bipartite_adjacency_matrix = create_bipartite_adjacency_matrix(data_model_train_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "propagation_maxLength = propagation_matrix_withWalkLength(bipartite_adjacency_matrix, 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "# 提取用户-商品传播矩阵和商品-用户传播矩阵\n",
    "n_users = data_model_train_matrix.shape[0]\n",
    "user_item_propagation = propagation_maxLength[:n_users, n_users:]\n",
    "item_user_propagation = propagation_maxLength[n_users:, :n_users]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [],
   "source": [
    "# 计算 Random Walk Kernel\n",
    "random_walk_kernel = np.dot(user_item_propagation, item_user_propagation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "def normalize_kernel(kernel_matrix):\n",
    "    diagonal_elements = np.diag(kernel_matrix)\n",
    "    normalized_kernel_matrix = np.divide(kernel_matrix, np.sqrt(np.outer(diagonal_elements, diagonal_elements)))\n",
    "    return normalized_kernel_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "normalized_kernel = normalize_kernel(random_walk_kernel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [],
   "source": [
    "normalized_kernel = pd.DataFrame(normalized_kernel,index=datasets_train,columns=datasets_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [],
   "source": [
    "lambda_ = 0.5\n",
    "for i in datasets_train:\n",
    "    for j in datasets_train:\n",
    "        # rating_based_sim = cosine_similarity_func(data_model_train_matrix,i,j)\n",
    "        if normalized_kernel.loc[i][j] != 0 and meta_dataset_similarity.loc[i][j] != 0:\n",
    "            dataset_similarity.loc[i][j] = lambda_ * normalized_kernel.loc[i][j] + (1-lambda_) * meta_dataset_similarity.loc[i][j]\n",
    "        else:\n",
    "            dataset_similarity.loc[i][j] = 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "data": {
      "text/plain": "     0    2    4    5    6  7  8  9  10 11  ... 55 56 57 58 59   60   61   62  \\\n0   1.0    0    0    0    0  0  0  0  0  0  ...  0  0  0  0  0    0    0    0   \n2     0  1.0    0    0    0  0  0  0  0  0  ...  0  0  0  0  0    0    0    0   \n4     0    0  1.0    0    0  0  0  0  0  0  ...  0  0  0  0  0    0    0    0   \n5     0    0    0  1.0    0  0  0  0  0  0  ...  0  0  0  0  0    0    0    0   \n6     0    0    0    0  1.0  0  0  0  0  0  ...  0  0  0  0  0    0    0    0   \n..  ...  ...  ...  ...  ... .. .. .. .. ..  ... .. .. .. .. ..  ...  ...  ...   \n60    0    0    0    0    0  0  0  0  0  0  ...  0  0  0  0  0  1.0    0    0   \n61    0    0    0    0    0  0  0  0  0  0  ...  0  0  0  0  0    0  1.0    0   \n62    0    0    0    0    0  0  0  0  0  0  ...  0  0  0  0  0    0    0  1.0   \n64    0    0    0    0    0  0  0  0  0  0  ...  0  0  0  0  0    0    0    0   \n65    0    0    0    0    0  0  0  0  0  0  ...  0  0  0  0  0    0    0    0   \n\n     64   65  \n0     0    0  \n2     0    0  \n4     0    0  \n5     0    0  \n6     0    0  \n..  ...  ...  \n60    0    0  \n61    0    0  \n62    0    0  \n64  1.0    0  \n65    0  1.0  \n\n[62 rows x 62 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>2</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>...</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n      <th>60</th>\n      <th>61</th>\n      <th>62</th>\n      <th>64</th>\n      <th>65</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>62 rows × 62 columns</p>\n</div>"
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_similarity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "def predict_ratings(rating_matrix, user_similarity_matrix, k=5):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "    rating_matrix - 评分矩阵，DataFrame格式，其中NaN表示未评分\n",
    "    user_similarity_matrix - 用户相似度矩阵，DataFrame格式\n",
    "    k - 最近邻的数量，默认为5\n",
    "\n",
    "    输出：\n",
    "    prediction_matrix - 预测矩阵，DataFrame格式\n",
    "    \"\"\"\n",
    "\n",
    "    # 初始化预测矩阵\n",
    "    prediction_matrix = rating_matrix.copy()\n",
    "\n",
    "    # 对于评分矩阵中的每个NaN值，使用K最近邻的方法预测评分\n",
    "    for i in rating_matrix.index:\n",
    "        for j in rating_matrix.columns:\n",
    "            if np.isnan(rating_matrix.loc[i][j]):\n",
    "                # 获取第i个用户的相似度值，并在相似度矩阵中找到K个最相似的用户\n",
    "                similarity_values = user_similarity_matrix.loc[i].sort_values(ascending=False)[1:k+1]\n",
    "\n",
    "                # 计算加权平均评分\n",
    "                weighted_sum = 0\n",
    "                similarity_sum = 0\n",
    "                for index, value in similarity_values.items():\n",
    "                    user_rating = rating_matrix.loc[index][j]\n",
    "                    if not np.isnan(user_rating):\n",
    "                        weighted_sum += value * user_rating\n",
    "                        similarity_sum += value\n",
    "\n",
    "                # 如果存在至少一个相似用户对该物品进行了评分，则计算预测评分\n",
    "                if similarity_sum != 0:\n",
    "                    prediction_matrix.loc[i][j] = weighted_sum / similarity_sum\n",
    "                else:\n",
    "                    # 如果没有相似用户评分，则使用当前用户的平均评分作为预测值\n",
    "                    prediction_matrix.loc[i][j] = rating_matrix.loc[i].mean()\n",
    "\n",
    "    return prediction_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "model_prediction_train = predict_ratings(data_model_train_matrix,dataset_similarity)\n",
    "model_prediction_train = pd.DataFrame(model_prediction_train,index=datasets_train,columns=models).sort_index().sort_index(axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "data": {
      "text/plain": "         66        67        68        69        70        71        72   \\\n0     0.9385    0.9385    0.9385    0.9385    0.9385    0.9385    0.9385   \n2     0.8075    0.8075    0.8075    0.8075    0.8075    0.8075    0.8075   \n4   0.767278  0.767278  0.767278  0.767278  0.767278  0.767278  0.767278   \n5       0.48      0.48      0.48      0.48      0.48      0.48      0.48   \n6      0.818     0.818     0.818     0.818     0.818     0.818     0.818   \n..       ...       ...       ...       ...       ...       ...       ...   \n60      0.74      0.74      0.74      0.74      0.74      0.74      0.74   \n61     0.733     0.733     0.733     0.733     0.733     0.733     0.733   \n62      0.75      0.75      0.75      0.75      0.75      0.75      0.75   \n64  0.641448  0.641448  0.641448  0.641448  0.641448  0.641448  0.641448   \n65    0.8975    0.8975    0.8975    0.8975    0.8975    0.8975    0.8975   \n\n         73        74        75   ...       988       989       990       991  \\\n0     0.9385    0.9385    0.9385  ...    0.9385    0.9385    0.9385    0.9385   \n2     0.8075    0.8075    0.8075  ...    0.8075    0.8075    0.8075    0.8075   \n4   0.767278  0.767278  0.767278  ...  0.767278  0.767278  0.767278  0.767278   \n5       0.48      0.48      0.48  ...      0.48      0.48      0.48      0.48   \n6      0.818     0.818     0.818  ...     0.818     0.818     0.818     0.818   \n..       ...       ...       ...  ...       ...       ...       ...       ...   \n60      0.74      0.74      0.74  ...      0.74      0.74      0.74      0.74   \n61     0.733     0.733     0.733  ...     0.733     0.733     0.733     0.733   \n62      0.75      0.75      0.75  ...      0.75      0.75      0.75      0.75   \n64  0.641448  0.641448  0.641448  ...  0.641448  0.641448  0.641448  0.641448   \n65    0.8975    0.8975    0.8975  ...    0.8975    0.8975    0.8975    0.8975   \n\n         992       993       994       995       996       997  \n0     0.9385    0.9385    0.9385    0.9385    0.9385    0.9385  \n2     0.8075    0.8075    0.8075    0.8075    0.8075    0.8075  \n4   0.767278  0.767278  0.767278  0.767278  0.767278  0.767278  \n5       0.48      0.48      0.48      0.48      0.48      0.48  \n6      0.818     0.818     0.818     0.818     0.818     0.818  \n..       ...       ...       ...       ...       ...       ...  \n60      0.74      0.74      0.74      0.74      0.74      0.74  \n61     0.733     0.733     0.733     0.733     0.733     0.733  \n62      0.75      0.75      0.75      0.75      0.75      0.75  \n64  0.641448  0.641448  0.641448  0.641448  0.641448  0.641448  \n65    0.8975    0.8975    0.8975    0.8975    0.8975    0.8975  \n\n[62 rows x 932 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>66</th>\n      <th>67</th>\n      <th>68</th>\n      <th>69</th>\n      <th>70</th>\n      <th>71</th>\n      <th>72</th>\n      <th>73</th>\n      <th>74</th>\n      <th>75</th>\n      <th>...</th>\n      <th>988</th>\n      <th>989</th>\n      <th>990</th>\n      <th>991</th>\n      <th>992</th>\n      <th>993</th>\n      <th>994</th>\n      <th>995</th>\n      <th>996</th>\n      <th>997</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>...</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n      <td>0.9385</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>...</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n      <td>0.8075</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>...</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n      <td>0.767278</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>...</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n      <td>0.48</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>...</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n      <td>0.818</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>...</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n      <td>0.74</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>...</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n      <td>0.733</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>...</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>...</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n      <td>0.641448</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>...</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n      <td>0.8975</td>\n    </tr>\n  </tbody>\n</table>\n<p>62 rows × 932 columns</p>\n</div>"
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_prediction_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "model_prediction_test = pd.DataFrame(index=datasets_test,columns=model_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "data": {
      "text/plain": "25.815704584121704"
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time_train = time.time()\n",
    "Training_time = end_time_train - start_time_train\n",
    "Training_time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "def find_sim_index(index):\n",
    "    row1 = meta_dataset_similarity.loc[index]\n",
    "    row1_max_index = row1[row1 == row1.max()].index[0]\n",
    "    return row1_max_index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [],
   "source": [
    "def Find_Top_k(i,sim_matrix):\n",
    "    row = sim_matrix.loc[i]\n",
    "    row = row.sort_values(ascending=False)\n",
    "    index_row = row.index\n",
    "    index_row = index_row.values.tolist()\n",
    "    return index_row"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [],
   "source": [
    "for dataset in datasets_test:\n",
    "    for model in model_test:\n",
    "        dataset_sim_list = Find_Top_k(dataset,meta_dataset_similarity)[1:]\n",
    "        # 仅保留存在于 model_prediction_train 的索引\n",
    "        valid_indices = [idx for idx in dataset_sim_list if idx in model_prediction_train.index][:15]\n",
    "        model_prediction_test.loc[dataset][model] = model_prediction_train.loc[valid_indices][model].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "data": {
      "text/plain": "[64, 54, 51, 37, 39, 9, 4, 32, 40, 43, 62, 26, 13, 65, 44]"
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_indices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [
    {
     "data": {
      "text/plain": "0.12262892723083496"
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time - start_time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [],
   "source": [
    "for i in datasets_test:\n",
    "    for j in model_test:\n",
    "        if data_model_test_matrix.loc[i][j] == 0:\n",
    "            model_prediction_test.loc[i][j] = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns={\"dataset\",\"model\",\"according_accuracy\",\"groundtruth_according_accuracy\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [],
   "source": [
    "for i in datasets_test:\n",
    "    for j in model_test:\n",
    "        if model_prediction_test.loc[i][j] is not None:\n",
    "            according_accuracy = model_prediction_test.loc[i][j]\n",
    "            groundtruth_according_accuracy = data_model_test_matrix.loc[i][j]\n",
    "            result = result.append([{'dataset':i,'model':j,'according_accuracy':according_accuracy,'groundtruth_according_accuracy':groundtruth_according_accuracy}],ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [
    {
     "data": {
      "text/plain": "   dataset  according_accuracy  groundtruth_according_accuracy model\n0        1            0.834907                           0.949   596\n1        1            0.834907                           0.947   708\n2        1            0.834907                           0.946   795\n3        1            0.834907                           0.925   597\n4        1            0.846105                           0.914   950\n5        1            0.834907                           0.905   841\n6        1            0.834907                           0.901   641\n7        1            0.837810                           0.883   868\n8        1            0.833490                           0.851   772\n9        1            0.845078                           0.508   527\n10       1            0.834907                           0.501   318\n11       3            0.794402                           0.946   319\n12       3            0.794402                           0.922   177\n13       3            0.794402                           0.910   178\n14       3            0.779383                           0.886   675\n15       3            0.790772                           0.870   791\n16       3            0.794402                           0.807   277\n17       3            0.793067                           0.778   456\n18       3            0.765305                           0.502   978\n19       3            0.754534                           0.498   368\n20       3            0.796000                           0.497   857\n21      23            0.841992                           0.871   533\n22      23            0.846832                           0.834   872\n23      23            0.841992                           0.831   469\n24      23            0.841992                           0.782   850\n25      23            0.841992                           0.779   985\n26      23            0.841992                           0.777   995\n27      23            0.841992                           0.754   921\n28      23            0.841992                           0.724   997\n29      23            0.841992                           0.670   197\n30      23            0.841992                           0.071   600\n31      63            0.834523                           0.954   319\n32      63            0.857560                           0.974   784\n33      63            0.856627                           0.960   780\n34      63            0.855760                           0.959   783\n35      63            0.856227                           0.957   782\n36      63            0.843827                           0.952   323\n37      63            0.820427                           0.533   511\n38      63            0.834027                           0.509   517\n39      63            0.831227                           0.484   717\n40      63            0.829093                           0.474   885",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>according_accuracy</th>\n      <th>groundtruth_according_accuracy</th>\n      <th>model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.834907</td>\n      <td>0.949</td>\n      <td>596</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.834907</td>\n      <td>0.947</td>\n      <td>708</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0.834907</td>\n      <td>0.946</td>\n      <td>795</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0.834907</td>\n      <td>0.925</td>\n      <td>597</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0.846105</td>\n      <td>0.914</td>\n      <td>950</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>0.834907</td>\n      <td>0.905</td>\n      <td>841</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>0.834907</td>\n      <td>0.901</td>\n      <td>641</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>0.837810</td>\n      <td>0.883</td>\n      <td>868</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>0.833490</td>\n      <td>0.851</td>\n      <td>772</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>0.845078</td>\n      <td>0.508</td>\n      <td>527</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>0.834907</td>\n      <td>0.501</td>\n      <td>318</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3</td>\n      <td>0.794402</td>\n      <td>0.946</td>\n      <td>319</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>3</td>\n      <td>0.794402</td>\n      <td>0.922</td>\n      <td>177</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3</td>\n      <td>0.794402</td>\n      <td>0.910</td>\n      <td>178</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3</td>\n      <td>0.779383</td>\n      <td>0.886</td>\n      <td>675</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>3</td>\n      <td>0.790772</td>\n      <td>0.870</td>\n      <td>791</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>3</td>\n      <td>0.794402</td>\n      <td>0.807</td>\n      <td>277</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>3</td>\n      <td>0.793067</td>\n      <td>0.778</td>\n      <td>456</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>3</td>\n      <td>0.765305</td>\n      <td>0.502</td>\n      <td>978</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>3</td>\n      <td>0.754534</td>\n      <td>0.498</td>\n      <td>368</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>3</td>\n      <td>0.796000</td>\n      <td>0.497</td>\n      <td>857</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>23</td>\n      <td>0.841992</td>\n      <td>0.871</td>\n      <td>533</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>23</td>\n      <td>0.846832</td>\n      <td>0.834</td>\n      <td>872</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>0.841992</td>\n      <td>0.831</td>\n      <td>469</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>23</td>\n      <td>0.841992</td>\n      <td>0.782</td>\n      <td>850</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>23</td>\n      <td>0.841992</td>\n      <td>0.779</td>\n      <td>985</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>23</td>\n      <td>0.841992</td>\n      <td>0.777</td>\n      <td>995</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>23</td>\n      <td>0.841992</td>\n      <td>0.754</td>\n      <td>921</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>23</td>\n      <td>0.841992</td>\n      <td>0.724</td>\n      <td>997</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>23</td>\n      <td>0.841992</td>\n      <td>0.670</td>\n      <td>197</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>23</td>\n      <td>0.841992</td>\n      <td>0.071</td>\n      <td>600</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>63</td>\n      <td>0.834523</td>\n      <td>0.954</td>\n      <td>319</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>63</td>\n      <td>0.857560</td>\n      <td>0.974</td>\n      <td>784</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>63</td>\n      <td>0.856627</td>\n      <td>0.960</td>\n      <td>780</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>63</td>\n      <td>0.855760</td>\n      <td>0.959</td>\n      <td>783</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>63</td>\n      <td>0.856227</td>\n      <td>0.957</td>\n      <td>782</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>63</td>\n      <td>0.843827</td>\n      <td>0.952</td>\n      <td>323</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>63</td>\n      <td>0.820427</td>\n      <td>0.533</td>\n      <td>511</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>63</td>\n      <td>0.834027</td>\n      <td>0.509</td>\n      <td>517</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>63</td>\n      <td>0.831227</td>\n      <td>0.484</td>\n      <td>717</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>63</td>\n      <td>0.829093</td>\n      <td>0.474</td>\n      <td>885</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [],
   "source": [
    "result.to_csv(\"../Huggingface/Output/Dataset_RandomWalk/Full_Dataset_RandomWalk@6@15.csv\",index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "23fcb16ef9ae263cc1ee2ef7013048b59283f261690a66bd73349f654cd13bd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}