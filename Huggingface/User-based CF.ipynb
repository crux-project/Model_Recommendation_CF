{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ratings_train = pd.read_csv(\"./Data/rate_train.csv\", low_memory=False)\n",
    "ratings_test = pd.read_csv(\"./Data/rate_test.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets_train = ratings_train.dataset_id.unique()\n",
    "model_train = ratings_train.model_id.unique()\n",
    "datasets_test = ratings_test.dataset_id.unique()\n",
    "model_test = ratings_test.model_id.unique()\n",
    "meta_models = pd.read_csv(\"./Data/models_v.csv\",low_memory=False)\n",
    "models = meta_models.model_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Rating Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_model_train_matrix = pd.DataFrame(index=datasets_train,columns=models)\n",
    "data_model_test_matrix = pd.DataFrame(index=datasets_test,columns=model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for row in ratings_train.itertuples():\n",
    "    data_model_train_matrix.loc[row[1]][row[2]] = row[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for row in ratings_test.itertuples():\n",
    "    data_model_test_matrix.loc[row[1]][row[2]] = row[3]\n",
    "data_model_test_matrix = data_model_test_matrix.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dataset Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "meta_datasets = pd.read_csv(\"./Data/dataset_v.csv\",low_memory=False)\n",
    "datasets = meta_datasets.dataset_id.unique()\n",
    "meta_datasets = meta_datasets.loc[:,(\"v1\",\"v2\",\"v3\",\"v4\",\"v5\",\"v6\",\"v7\",\"v8\",\"v9\",\"v10\",\"v11\",\"v12\",\"v13\",\"v14\",\"v15\",\"v16\")]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 对dataframe的数据进行标准化\n",
    "scaled_data = scaler.fit_transform(meta_datasets)\n",
    "# 将标准化后的数据转换为dataframe，并保留原始索引\n",
    "scaled_df = pd.DataFrame(scaled_data, index=meta_datasets.index, columns=meta_datasets.columns)\n",
    "meta_dataset_similarity = cosine_similarity(scaled_df.values.tolist())\n",
    "meta_dataset_similarity = pd.DataFrame(meta_dataset_similarity,index=datasets,columns=datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          0         1         2         3         4         5         6   \\\n0   1.000000  0.057282 -0.205998  0.145192  0.020640  0.110559  0.097026   \n1   0.057282  1.000000 -0.438958  0.578454  0.779882  0.028973  0.540947   \n2  -0.205998 -0.438958  1.000000 -0.210408 -0.600007  0.173321 -0.136983   \n3   0.145192  0.578454 -0.210408  1.000000  0.625829  0.710368  0.452639   \n4   0.020640  0.779882 -0.600007  0.625829  1.000000  0.236301  0.597579   \n..       ...       ...       ...       ...       ...       ...       ...   \n61  0.000690  0.072043  0.018348  0.172264 -0.101544  0.063448  0.190238   \n62  0.002975  0.483767 -0.653393  0.019105  0.550476 -0.345970  0.390821   \n63  0.011482  0.635354 -0.674113  0.389424  0.686314 -0.064017  0.286442   \n64 -0.021570  0.361997 -0.538035  0.334551  0.649043  0.118673  0.337409   \n65  0.163135  0.187997 -0.491216 -0.314283  0.212394 -0.448983 -0.041334   \n\n          7         8         9   ...        56        57        58        59  \\\n0   0.276500 -0.351863 -0.047295  ...  0.149719 -0.409445  0.263787  0.115303   \n1  -0.554238  0.296891  0.523141  ... -0.539426  0.291139 -0.119987  0.288116   \n2  -0.064347 -0.089264 -0.570692  ...  0.383651 -0.332607 -0.106987 -0.151404   \n3  -0.330582  0.063678  0.173019  ... -0.340678 -0.067492 -0.165847  0.162780   \n4  -0.284183  0.279180  0.321405  ... -0.711128  0.226895 -0.218268  0.320114   \n..       ...       ...       ...  ...       ...       ...       ...       ...   \n61  0.121590 -0.332661 -0.127737  ...  0.253976  0.207105 -0.263444 -0.055246   \n62 -0.186024  0.003978  0.338140  ... -0.500291  0.411823 -0.031752  0.424013   \n63 -0.343642  0.024739  0.696643  ... -0.494205  0.274190 -0.149976 -0.010252   \n64  0.009436 -0.106836  0.386334  ... -0.471695  0.192723 -0.216473 -0.075260   \n65 -0.019379 -0.110797  0.423570  ... -0.276432  0.072177  0.194846  0.015652   \n\n          60        61        62        63        64        65  \n0  -0.013954  0.000690  0.002975  0.011482 -0.021570  0.163135  \n1  -0.037878  0.072043  0.483767  0.635354  0.361997  0.187997  \n2  -0.029464  0.018348 -0.653393 -0.674113 -0.538035 -0.491216  \n3  -0.026382  0.172264  0.019105  0.389424  0.334551 -0.314283  \n4  -0.221069 -0.101544  0.550476  0.686314  0.649043  0.212394  \n..       ...       ...       ...       ...       ...       ...  \n61  0.326315  1.000000  0.168447 -0.087512 -0.035440 -0.183921  \n62  0.178791  0.168447  1.000000  0.542431  0.506908  0.649412  \n63  0.115874 -0.087512  0.542431  1.000000  0.828097  0.472056  \n64  0.214415 -0.035440  0.506908  0.828097  1.000000  0.523058  \n65  0.424905 -0.183921  0.649412  0.472056  0.523058  1.000000  \n\n[66 rows x 66 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n      <th>60</th>\n      <th>61</th>\n      <th>62</th>\n      <th>63</th>\n      <th>64</th>\n      <th>65</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.000000</td>\n      <td>0.057282</td>\n      <td>-0.205998</td>\n      <td>0.145192</td>\n      <td>0.020640</td>\n      <td>0.110559</td>\n      <td>0.097026</td>\n      <td>0.276500</td>\n      <td>-0.351863</td>\n      <td>-0.047295</td>\n      <td>...</td>\n      <td>0.149719</td>\n      <td>-0.409445</td>\n      <td>0.263787</td>\n      <td>0.115303</td>\n      <td>-0.013954</td>\n      <td>0.000690</td>\n      <td>0.002975</td>\n      <td>0.011482</td>\n      <td>-0.021570</td>\n      <td>0.163135</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.057282</td>\n      <td>1.000000</td>\n      <td>-0.438958</td>\n      <td>0.578454</td>\n      <td>0.779882</td>\n      <td>0.028973</td>\n      <td>0.540947</td>\n      <td>-0.554238</td>\n      <td>0.296891</td>\n      <td>0.523141</td>\n      <td>...</td>\n      <td>-0.539426</td>\n      <td>0.291139</td>\n      <td>-0.119987</td>\n      <td>0.288116</td>\n      <td>-0.037878</td>\n      <td>0.072043</td>\n      <td>0.483767</td>\n      <td>0.635354</td>\n      <td>0.361997</td>\n      <td>0.187997</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.205998</td>\n      <td>-0.438958</td>\n      <td>1.000000</td>\n      <td>-0.210408</td>\n      <td>-0.600007</td>\n      <td>0.173321</td>\n      <td>-0.136983</td>\n      <td>-0.064347</td>\n      <td>-0.089264</td>\n      <td>-0.570692</td>\n      <td>...</td>\n      <td>0.383651</td>\n      <td>-0.332607</td>\n      <td>-0.106987</td>\n      <td>-0.151404</td>\n      <td>-0.029464</td>\n      <td>0.018348</td>\n      <td>-0.653393</td>\n      <td>-0.674113</td>\n      <td>-0.538035</td>\n      <td>-0.491216</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.145192</td>\n      <td>0.578454</td>\n      <td>-0.210408</td>\n      <td>1.000000</td>\n      <td>0.625829</td>\n      <td>0.710368</td>\n      <td>0.452639</td>\n      <td>-0.330582</td>\n      <td>0.063678</td>\n      <td>0.173019</td>\n      <td>...</td>\n      <td>-0.340678</td>\n      <td>-0.067492</td>\n      <td>-0.165847</td>\n      <td>0.162780</td>\n      <td>-0.026382</td>\n      <td>0.172264</td>\n      <td>0.019105</td>\n      <td>0.389424</td>\n      <td>0.334551</td>\n      <td>-0.314283</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.020640</td>\n      <td>0.779882</td>\n      <td>-0.600007</td>\n      <td>0.625829</td>\n      <td>1.000000</td>\n      <td>0.236301</td>\n      <td>0.597579</td>\n      <td>-0.284183</td>\n      <td>0.279180</td>\n      <td>0.321405</td>\n      <td>...</td>\n      <td>-0.711128</td>\n      <td>0.226895</td>\n      <td>-0.218268</td>\n      <td>0.320114</td>\n      <td>-0.221069</td>\n      <td>-0.101544</td>\n      <td>0.550476</td>\n      <td>0.686314</td>\n      <td>0.649043</td>\n      <td>0.212394</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>0.000690</td>\n      <td>0.072043</td>\n      <td>0.018348</td>\n      <td>0.172264</td>\n      <td>-0.101544</td>\n      <td>0.063448</td>\n      <td>0.190238</td>\n      <td>0.121590</td>\n      <td>-0.332661</td>\n      <td>-0.127737</td>\n      <td>...</td>\n      <td>0.253976</td>\n      <td>0.207105</td>\n      <td>-0.263444</td>\n      <td>-0.055246</td>\n      <td>0.326315</td>\n      <td>1.000000</td>\n      <td>0.168447</td>\n      <td>-0.087512</td>\n      <td>-0.035440</td>\n      <td>-0.183921</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>0.002975</td>\n      <td>0.483767</td>\n      <td>-0.653393</td>\n      <td>0.019105</td>\n      <td>0.550476</td>\n      <td>-0.345970</td>\n      <td>0.390821</td>\n      <td>-0.186024</td>\n      <td>0.003978</td>\n      <td>0.338140</td>\n      <td>...</td>\n      <td>-0.500291</td>\n      <td>0.411823</td>\n      <td>-0.031752</td>\n      <td>0.424013</td>\n      <td>0.178791</td>\n      <td>0.168447</td>\n      <td>1.000000</td>\n      <td>0.542431</td>\n      <td>0.506908</td>\n      <td>0.649412</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>0.011482</td>\n      <td>0.635354</td>\n      <td>-0.674113</td>\n      <td>0.389424</td>\n      <td>0.686314</td>\n      <td>-0.064017</td>\n      <td>0.286442</td>\n      <td>-0.343642</td>\n      <td>0.024739</td>\n      <td>0.696643</td>\n      <td>...</td>\n      <td>-0.494205</td>\n      <td>0.274190</td>\n      <td>-0.149976</td>\n      <td>-0.010252</td>\n      <td>0.115874</td>\n      <td>-0.087512</td>\n      <td>0.542431</td>\n      <td>1.000000</td>\n      <td>0.828097</td>\n      <td>0.472056</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>-0.021570</td>\n      <td>0.361997</td>\n      <td>-0.538035</td>\n      <td>0.334551</td>\n      <td>0.649043</td>\n      <td>0.118673</td>\n      <td>0.337409</td>\n      <td>0.009436</td>\n      <td>-0.106836</td>\n      <td>0.386334</td>\n      <td>...</td>\n      <td>-0.471695</td>\n      <td>0.192723</td>\n      <td>-0.216473</td>\n      <td>-0.075260</td>\n      <td>0.214415</td>\n      <td>-0.035440</td>\n      <td>0.506908</td>\n      <td>0.828097</td>\n      <td>1.000000</td>\n      <td>0.523058</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>0.163135</td>\n      <td>0.187997</td>\n      <td>-0.491216</td>\n      <td>-0.314283</td>\n      <td>0.212394</td>\n      <td>-0.448983</td>\n      <td>-0.041334</td>\n      <td>-0.019379</td>\n      <td>-0.110797</td>\n      <td>0.423570</td>\n      <td>...</td>\n      <td>-0.276432</td>\n      <td>0.072177</td>\n      <td>0.194846</td>\n      <td>0.015652</td>\n      <td>0.424905</td>\n      <td>-0.183921</td>\n      <td>0.649412</td>\n      <td>0.472056</td>\n      <td>0.523058</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>66 rows × 66 columns</p>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_dataset_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_func(ratings, user1, user2):\n",
    "    # 找到两个用户共同评分的物品，并将这些评分放入一个向量中\n",
    "    u1_ratings = ratings.loc[user1].dropna()\n",
    "    u2_ratings = ratings.loc[user2].dropna()\n",
    "\n",
    "    common_items = np.intersect1d(u1_ratings.index, u2_ratings.index).tolist()\n",
    "    u1_common_ratings = u1_ratings.loc[common_items]\n",
    "    u2_common_ratings = u2_ratings.loc[common_items]\n",
    "\n",
    "    # 计算两个向量之间的余弦相似度\n",
    "    if len(common_items) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        cos_sim = np.dot(u1_common_ratings, u2_common_ratings) / (np.linalg.norm(u1_common_ratings) * np.linalg.norm(u2_common_ratings))\n",
    "        return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_similarity = pd.DataFrame(index=datasets_train,columns=datasets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lambda_ = 0.5\n",
    "for i in datasets_train:\n",
    "    for j in datasets_train:\n",
    "        rating_based_sim = cosine_similarity_func(data_model_train_matrix,i,j)\n",
    "        if rating_based_sim!= 0:\n",
    "            dataset_similarity.loc[i][j] = rating_based_sim\n",
    "            continue\n",
    "        else:\n",
    "            dataset_similarity.loc[i][j] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     0    2    4    5    6  7  8  9  10 11  ... 55 56 57 58 59   60   61   62  \\\n0   1.0    0    0    0    0  0  0  0  0  0  ...  0  0  0  0  0    0    0    0   \n2     0  1.0    0    0    0  0  0  0  0  0  ...  0  0  0  0  0    0    0    0   \n4     0    0  1.0    0    0  0  0  0  0  0  ...  0  0  0  0  0    0    0    0   \n5     0    0    0  1.0    0  0  0  0  0  0  ...  0  0  0  0  0    0    0    0   \n6     0    0    0    0  1.0  0  0  0  0  0  ...  0  0  0  0  0    0    0    0   \n..  ...  ...  ...  ...  ... .. .. .. .. ..  ... .. .. .. .. ..  ...  ...  ...   \n60    0    0    0    0    0  0  0  0  0  0  ...  0  0  0  0  0  1.0    0    0   \n61    0    0    0    0    0  0  0  0  0  0  ...  0  0  0  0  0    0  1.0    0   \n62    0    0    0    0    0  0  0  0  0  0  ...  0  0  0  0  0    0    0  1.0   \n64    0    0    0    0    0  0  0  0  0  0  ...  0  0  0  0  0    0    0    0   \n65    0    0    0    0    0  0  0  0  0  0  ...  0  0  0  0  0    0    0    0   \n\n     64   65  \n0     0    0  \n2     0    0  \n4     0    0  \n5     0    0  \n6     0    0  \n..  ...  ...  \n60    0    0  \n61    0    0  \n62    0    0  \n64  1.0    0  \n65    0  1.0  \n\n[62 rows x 62 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>2</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>...</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n      <th>60</th>\n      <th>61</th>\n      <th>62</th>\n      <th>64</th>\n      <th>65</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>62 rows × 62 columns</p>\n</div>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(rating_matrix, similarity_matrix):\n",
    "    \"\"\"\n",
    "    根据评分矩阵和相似度矩阵预测评分。\n",
    "\n",
    "    参数：\n",
    "    rating_matrix (pd.DataFrame)：评分矩阵，包含NaN值\n",
    "    similarity_matrix (numpy.array)：相似度矩阵\n",
    "\n",
    "    返回：\n",
    "    pd.DataFrame：预测评分矩阵\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取评分矩阵的均值（忽略NaN值）\n",
    "    mean_rating = rating_matrix.mean(axis=1).values\n",
    "\n",
    "    # 将评分矩阵中的NaN值替换为0\n",
    "    rating_matrix_nan_to_zero = rating_matrix.fillna(0).values\n",
    "\n",
    "    # 减去均值，得到归一化的评分矩阵\n",
    "    normalized_rating_matrix = rating_matrix_nan_to_zero - mean_rating[:, np.newaxis]\n",
    "\n",
    "    # 计算预测评分\n",
    "    predicted_ratings = mean_rating[:, np.newaxis] + np.dot(similarity_matrix, normalized_rating_matrix) / np.abs(similarity_matrix).sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # 将预测评分数组转换为DataFrame\n",
    "    predicted_ratings_df = pd.DataFrame(predicted_ratings, index=rating_matrix.index, columns=rating_matrix.columns)\n",
    "\n",
    "    return predicted_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_ratings(rating_matrix, user_similarity_matrix, k=5):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "    rating_matrix - 评分矩阵，DataFrame格式，其中NaN表示未评分\n",
    "    user_similarity_matrix - 用户相似度矩阵，DataFrame格式\n",
    "    k - 最近邻的数量，默认为5\n",
    "\n",
    "    输出：\n",
    "    prediction_matrix - 预测矩阵，DataFrame格式\n",
    "    \"\"\"\n",
    "\n",
    "    # 初始化预测矩阵\n",
    "    prediction_matrix = rating_matrix.copy()\n",
    "\n",
    "    # 对于评分矩阵中的每个NaN值，使用K最近邻的方法预测评分\n",
    "    for i in rating_matrix.index:\n",
    "        for j in rating_matrix.columns:\n",
    "            if np.isnan(rating_matrix.loc[i][j]):\n",
    "                # 获取第i个用户的相似度值，并在相似度矩阵中找到K个最相似的用户\n",
    "                similarity_values = user_similarity_matrix.loc[i].sort_values(ascending=False)[1:k+1]\n",
    "\n",
    "                # 计算加权平均评分\n",
    "                weighted_sum = 0\n",
    "                similarity_sum = 0\n",
    "                for index, value in similarity_values.items():\n",
    "                    user_rating = rating_matrix.loc[index][j]\n",
    "                    if not np.isnan(user_rating):\n",
    "                        weighted_sum += value * user_rating\n",
    "                        similarity_sum += value\n",
    "\n",
    "                # 如果存在至少一个相似用户对该物品进行了评分，则计算预测评分\n",
    "                if similarity_sum != 0:\n",
    "                    prediction_matrix.loc[i][j] = weighted_sum / similarity_sum\n",
    "                else:\n",
    "                    # 如果没有相似用户评分，则使用当前用户的平均评分作为预测值\n",
    "                    prediction_matrix.loc[i][j] = rating_matrix.loc[i].mean()\n",
    "\n",
    "    return prediction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_prediction_train = predict_ratings(data_model_train_matrix,dataset_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_prediction_train = pd.DataFrame(model_prediction_train,index=datasets_train,columns=models).sort_index().sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_prediction_test = pd.DataFrame(index=datasets_test,columns=model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "end_time_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "26.968526601791382"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Training_time = end_time_train - start_time_train\n",
    "Training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Metadata Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_sim_index(index):\n",
    "    row1 = meta_dataset_similarity.loc[index]\n",
    "    row1_max_index = row1[row1 == row1.max()].index[0]\n",
    "    return row1_max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Find_Top_k(i,sim_matrix):\n",
    "    row = sim_matrix.loc[i]\n",
    "    row = row.sort_values(ascending=False)\n",
    "    index_row = row.index\n",
    "    index_row = index_row.values.tolist()\n",
    "    return index_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for dataset in datasets_test:\n",
    "    for model in model_test:\n",
    "        dataset_sim_list = Find_Top_k(dataset,meta_dataset_similarity)[1:]\n",
    "        # 仅保留存在于 model_prediction_train 的索引\n",
    "        valid_indices = [idx for idx in dataset_sim_list if idx in model_prediction_train.index][:15]\n",
    "        model_prediction_test.loc[dataset][model] = model_prediction_train.loc[valid_indices][model].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[64, 54, 51, 37, 39, 9, 4, 32, 40, 43, 62, 26, 13, 65, 44]"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.12468528747558594"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in datasets_test:\n",
    "    for j in model_test:\n",
    "        if data_model_test_matrix.loc[i][j] == 0:\n",
    "            model_prediction_test.loc[i][j] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns={\"dataset\",\"model\",\"according_accuracy\",\"groundtruth_according_accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in datasets_test:\n",
    "    for j in model_test:\n",
    "        if model_prediction_test.loc[i][j] is not None:\n",
    "            according_accuracy = model_prediction_test.loc[i][j]\n",
    "            groundtruth_according_accuracy = data_model_test_matrix.loc[i][j]\n",
    "            result = result.append([{'dataset':i,'model':j,'according_accuracy':according_accuracy,'groundtruth_according_accuracy':groundtruth_according_accuracy}],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   dataset  groundtruth_according_accuracy model  according_accuracy\n0        1                           0.949   596            0.834907\n1        1                           0.947   708            0.834907\n2        1                           0.946   795            0.834907\n3        1                           0.925   597            0.834907\n4        1                           0.914   950            0.846105\n5        1                           0.905   841            0.834907\n6        1                           0.901   641            0.834907\n7        1                           0.883   868            0.837810\n8        1                           0.851   772            0.833490\n9        1                           0.508   527            0.845078\n10       1                           0.501   318            0.834907\n11       3                           0.946   319            0.794402\n12       3                           0.922   177            0.794402\n13       3                           0.910   178            0.794402\n14       3                           0.886   675            0.779383\n15       3                           0.870   791            0.790772\n16       3                           0.807   277            0.794402\n17       3                           0.778   456            0.793067\n18       3                           0.502   978            0.765305\n19       3                           0.498   368            0.754534\n20       3                           0.497   857            0.796000\n21      23                           0.871   533            0.841992\n22      23                           0.834   872            0.846832\n23      23                           0.831   469            0.841992\n24      23                           0.782   850            0.841992\n25      23                           0.779   985            0.841992\n26      23                           0.777   995            0.841992\n27      23                           0.754   921            0.841992\n28      23                           0.724   997            0.841992\n29      23                           0.670   197            0.841992\n30      23                           0.071   600            0.841992\n31      63                           0.954   319            0.834523\n32      63                           0.974   784            0.857560\n33      63                           0.960   780            0.856627\n34      63                           0.959   783            0.855760\n35      63                           0.957   782            0.856227\n36      63                           0.952   323            0.843827\n37      63                           0.533   511            0.820427\n38      63                           0.509   517            0.834027\n39      63                           0.484   717            0.831227\n40      63                           0.474   885            0.829093",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>groundtruth_according_accuracy</th>\n      <th>model</th>\n      <th>according_accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.949</td>\n      <td>596</td>\n      <td>0.834907</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.947</td>\n      <td>708</td>\n      <td>0.834907</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0.946</td>\n      <td>795</td>\n      <td>0.834907</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0.925</td>\n      <td>597</td>\n      <td>0.834907</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0.914</td>\n      <td>950</td>\n      <td>0.846105</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>0.905</td>\n      <td>841</td>\n      <td>0.834907</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>0.901</td>\n      <td>641</td>\n      <td>0.834907</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>0.883</td>\n      <td>868</td>\n      <td>0.837810</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>0.851</td>\n      <td>772</td>\n      <td>0.833490</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>0.508</td>\n      <td>527</td>\n      <td>0.845078</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>0.501</td>\n      <td>318</td>\n      <td>0.834907</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3</td>\n      <td>0.946</td>\n      <td>319</td>\n      <td>0.794402</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>3</td>\n      <td>0.922</td>\n      <td>177</td>\n      <td>0.794402</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3</td>\n      <td>0.910</td>\n      <td>178</td>\n      <td>0.794402</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3</td>\n      <td>0.886</td>\n      <td>675</td>\n      <td>0.779383</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>3</td>\n      <td>0.870</td>\n      <td>791</td>\n      <td>0.790772</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>3</td>\n      <td>0.807</td>\n      <td>277</td>\n      <td>0.794402</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>3</td>\n      <td>0.778</td>\n      <td>456</td>\n      <td>0.793067</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>3</td>\n      <td>0.502</td>\n      <td>978</td>\n      <td>0.765305</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>3</td>\n      <td>0.498</td>\n      <td>368</td>\n      <td>0.754534</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>3</td>\n      <td>0.497</td>\n      <td>857</td>\n      <td>0.796000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>23</td>\n      <td>0.871</td>\n      <td>533</td>\n      <td>0.841992</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>23</td>\n      <td>0.834</td>\n      <td>872</td>\n      <td>0.846832</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>0.831</td>\n      <td>469</td>\n      <td>0.841992</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>23</td>\n      <td>0.782</td>\n      <td>850</td>\n      <td>0.841992</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>23</td>\n      <td>0.779</td>\n      <td>985</td>\n      <td>0.841992</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>23</td>\n      <td>0.777</td>\n      <td>995</td>\n      <td>0.841992</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>23</td>\n      <td>0.754</td>\n      <td>921</td>\n      <td>0.841992</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>23</td>\n      <td>0.724</td>\n      <td>997</td>\n      <td>0.841992</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>23</td>\n      <td>0.670</td>\n      <td>197</td>\n      <td>0.841992</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>23</td>\n      <td>0.071</td>\n      <td>600</td>\n      <td>0.841992</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>63</td>\n      <td>0.954</td>\n      <td>319</td>\n      <td>0.834523</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>63</td>\n      <td>0.974</td>\n      <td>784</td>\n      <td>0.857560</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>63</td>\n      <td>0.960</td>\n      <td>780</td>\n      <td>0.856627</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>63</td>\n      <td>0.959</td>\n      <td>783</td>\n      <td>0.855760</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>63</td>\n      <td>0.957</td>\n      <td>782</td>\n      <td>0.856227</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>63</td>\n      <td>0.952</td>\n      <td>323</td>\n      <td>0.843827</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>63</td>\n      <td>0.533</td>\n      <td>511</td>\n      <td>0.820427</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>63</td>\n      <td>0.509</td>\n      <td>517</td>\n      <td>0.834027</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>63</td>\n      <td>0.484</td>\n      <td>717</td>\n      <td>0.831227</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>63</td>\n      <td>0.474</td>\n      <td>885</td>\n      <td>0.829093</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result.to_csv(\"../Huggingface/Output/Rating_only/Full_Rating_only@15.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "23fcb16ef9ae263cc1ee2ef7013048b59283f261690a66bd73349f654cd13bd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}